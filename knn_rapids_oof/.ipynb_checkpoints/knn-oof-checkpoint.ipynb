{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# def install_rapids():\n",
    "#     import sys\n",
    "#     !cp ../input/rapids/rapids.0.12.0 /opt/conda/envs/rapids.tar.gz\n",
    "#     !cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\n",
    "#     sys.path = [\"/opt/conda/envs/rapids/lib/python3.6/site-packages\"] + sys.path\n",
    "#     sys.path = [\"/opt/conda/envs/rapids/lib/python3.6\"] + sys.path\n",
    "#     sys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n",
    "#     !cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/\n",
    "# install_rapids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: - ^C\n",
      "failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "CondaError: KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c rapidsai -c nvidia -c conda-forge -c defaults rapids=0.13 python=3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.6\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy c-extensions failed.\n- Try uninstalling and reinstalling numpy.\n- If you have already done that, then:\n  1. Check that you expected to use Python3.7 from \"/opt/conda/bin/python\",\n     and that you have no directories in your PATH or PYTHONPATH that can\n     interfere with the Python and numpy version \"1.18.1\" you're trying to use.\n  2. If (1) looks fine, you can open a new issue at\n     https://github.com/numpy/numpy/issues.  Please include details on:\n     - how you installed Python\n     - how you installed numpy\n     - your operating system\n     - whether or not you have multiple versions of Python installed\n     - if you built from source, your compiler versions and ideally a build log\n\n- If you're working with a numpy git repository, try `git clean -xdf`\n  (removes all files not under version control) and rebuild numpy.\n\nNote: this error has many possible causes, so please don't comment on\nan existing issue about this - open a new one instead.\n\nOriginal error was: No module named 'numpy.core._multiarray_umath'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/numpy/core/multiarray.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_multiarray_umath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m from numpy.core._multiarray_umath import (\n\u001b[0m\u001b[1;32m      8\u001b[0m     add_docstring, implement_array_function, _get_implementing_args)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f27ff5697ae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.6/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \"\"\" % (sys.version_info[0], sys.version_info[1], sys.executable,\n\u001b[1;32m     53\u001b[0m         __version__, exc)\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0menvkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_added\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy c-extensions failed.\n- Try uninstalling and reinstalling numpy.\n- If you have already done that, then:\n  1. Check that you expected to use Python3.7 from \"/opt/conda/bin/python\",\n     and that you have no directories in your PATH or PYTHONPATH that can\n     interfere with the Python and numpy version \"1.18.1\" you're trying to use.\n  2. If (1) looks fine, you can open a new issue at\n     https://github.com/numpy/numpy/issues.  Please include details on:\n     - how you installed Python\n     - how you installed numpy\n     - your operating system\n     - whether or not you have multiple versions of Python installed\n     - if you built from source, your compiler versions and ideally a build log\n\n- If you're working with a numpy git repository, try `git clean -xdf`\n  (removes all files not under version control) and rebuild numpy.\n\nNote: this error has many possible causes, so please don't comment on\nan existing issue about this - open a new one instead.\n\nOriginal error was: No module named 'numpy.core._multiarray_umath'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from cuml.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "import cuml; cuml.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NearestNeighborsFeats(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self,  k_list, n_jobs=1, n_classes=None, n_neighbors=None, eps=1e-6):\n",
    "        self.n_jobs = n_jobs\n",
    "        self.k_list = k_list\n",
    "        self.batch_size = 100_000\n",
    "        if n_neighbors is None:\n",
    "            self.n_neighbors = max(k_list) \n",
    "        else:\n",
    "            self.n_neighbors = n_neighbors\n",
    "            \n",
    "        self.eps = eps        \n",
    "        self.n_classes_ = n_classes\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.NN = NearestNeighbors(n_neighbors=max(self.k_list))\n",
    "        self.NN.fit(X)        \n",
    "        self.y_train = y\n",
    "        self.n_classes = np.unique(y).shape[0] if self.n_classes_ is None else self.n_classes_\n",
    "        \n",
    "    def get_batches(self, batch_size, neighs_dist, neighs):\n",
    "        n = len(neighs_dist)\n",
    "        it = int(np.ceil(n/batch_size))\n",
    "\n",
    "        batches = [(neighs_dist[batch_size*i:batch_size*(i+1)],\n",
    "                    neighs[batch_size*i:batch_size*(i+1)]) for i in range(it)]\n",
    "        return batches\n",
    "        \n",
    "    def predict(self, X):       \n",
    "        neighs_dist, neighs = self.NN.kneighbors(X)\n",
    "        batches = self.get_batches(self.batch_size, neighs_dist, neighs)\n",
    "                \n",
    "        test_feats = []\n",
    "        if self.n_jobs == 1:\n",
    "            for i, (neighs_dist, neighs) in enumerate(batches):\n",
    "                print(f'Batch number {i}...')\n",
    "                test_feats.append(self.get_features_for_batch(neighs_dist, neighs))            \n",
    "#         with Pool(self.n_jobs) as p:\n",
    "#             test_feats = p.map(self.get_features_for_batch, batches)                       \n",
    "        return np.vstack(test_feats)\n",
    "        \n",
    "        \n",
    "    def get_features_for_batch(self, neighs_dist, neighs):\n",
    "        \n",
    "        neighs_y = self.y_train[neighs.astype(int)] \n",
    "        return_list = []\n",
    "        \n",
    "        def bincount(x):\n",
    "            return np.bincount(x, minlength=self.n_classes)\n",
    "        \n",
    "        print('1. Labels distribution')\n",
    "        for k in self.k_list:\n",
    "            feats = np.apply_along_axis(bincount, axis=1, arr=neighs_y[:,:k])\n",
    "            feats = feats/ feats.sum(axis=1, keepdims=True)\n",
    "            assert feats.shape == (neighs_dist.shape[0], self.n_classes)\n",
    "            return_list += [feats]\n",
    "            \n",
    "        print('2. Mean')\n",
    "        for k in self.k_list:\n",
    "            feats = np.mean(neighs_y[:,:k], axis=1, keepdims=True)\n",
    "            assert feats.shape == (neighs_dist.shape[0], 1)\n",
    "            return_list += [feats]\n",
    "\n",
    "        print('3. Quantilles')\n",
    "        q = np.arange(0.0, 1.1, 0.1)\n",
    "        for k in self.k_list:\n",
    "            feats = np.quantile(neighs_y[:,:k], q=q, axis=1).T\n",
    "            assert feats.shape == (neighs_dist.shape[0], 11)\n",
    "            return_list += [feats]\n",
    "        \n",
    "        knn_feats = np.hstack(return_list)\n",
    "        return knn_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 10) 101\n",
      "Batch number 0...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 1...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 2...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 3...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 4...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n"
     ]
    }
   ],
   "source": [
    "%% time\n",
    "X = np.random.randn(5_00_000, 10)\n",
    "y = np.random.randint(10, size=(5_00_000))\n",
    "nn_feats = NearestNeighborsFeats(k_list=[51, 101])\n",
    "nn_feats.fit(X, y)\n",
    "feats = nn_feats.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_KNN_FEATS = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Input, Dense, Add, Multiply, GRU, Bidirectional, LSTM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import losses, models, optimizers\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations and main hyperparammeters\n",
    "\n",
    "GROUP_BATCH_SIZE = 4000\n",
    "SEED = 321\n",
    "SPLITS = 5\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data Started...\n",
      "Reading and Normalizing Data Completed\n",
      "Creating Features\n",
      "Feature Engineering Started...\n",
      "Feature Engineering Completed...\n",
      "Training Wavenet model with 5 folds of GroupKFold Started...\n",
      "(5000000, 7) (2000000, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000000, 7) 101\n",
      "Batch number 0...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 1...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 2...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 3...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 4...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 5...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 6...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 7...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 8...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 9...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "(4000000, 7) 101\n",
      "Batch number 0...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 1...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 2...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 3...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 4...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 5...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 6...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 7...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 8...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 9...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "(4000000, 7) 101\n",
      "Batch number 0...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 1...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 2...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 3...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 4...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 5...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 6...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 7...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 8...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 9...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "(4000000, 7) 101\n",
      "Batch number 0...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 1...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 2...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 3...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 4...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 5...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 6...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 7...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 8...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 9...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "(4000000, 7) 101\n",
      "Batch number 0...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 1...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 2...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 3...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 4...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 5...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 6...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 7...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 8...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 9...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  9.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000000, 7) 101\n",
      "Batch number 0...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 1...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 2...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 3...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 4...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 5...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 6...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 7...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 8...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 9...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 10...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 11...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 12...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 13...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 14...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 15...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 16...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 17...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 18...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Batch number 19...\n",
      "1. Labels distribution\n",
      "2. Mean\n",
      "3. Quantilles\n",
      "Training completed. oof macro f1 score : 0.84830\n",
      "Training completed...\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "def read_data():\n",
    "    train = pd.read_csv('/kaggle/input/data-without-drift/train_clean.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n",
    "    test  = pd.read_csv('/kaggle/input/data-without-drift/test_clean.csv', dtype={'time': np.float32, 'signal': np.float32})\n",
    "    sub  = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv', dtype={'time': np.float32})\n",
    "\n",
    "    return train, test, sub\n",
    "\n",
    "\n",
    "\n",
    "# create batches of 4000 observations\n",
    "def batching(df, batch_size):\n",
    "    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n",
    "    df['group'] = df['group'].astype(np.uint16)\n",
    "    return df\n",
    "\n",
    "# normalize the data (standard scaler). We can also try other scalers for a better score!\n",
    "def normalize(train, test):\n",
    "    train_input_mean = train.signal.mean()\n",
    "    train_input_sigma = train.signal.std()\n",
    "    train['signal'] = (train.signal - train_input_mean) / train_input_sigma\n",
    "    test['signal'] = (test.signal - train_input_mean) / train_input_sigma\n",
    "    return train, test\n",
    "\n",
    "# get lead and lags features\n",
    "def lag_with_pct_change(df, windows):\n",
    "    for window in windows:    \n",
    "        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n",
    "        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n",
    "    return df\n",
    "\n",
    "# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\n",
    "def run_feat_engineering(df, batch_size):\n",
    "    # create batches\n",
    "    df = batching(df, batch_size = batch_size)\n",
    "    # create leads and lags (1, 2, 3 making them 6 features)\n",
    "    df = lag_with_pct_change(df, [1, 2, 3])\n",
    "    # create signal ** 2 (this is the new feature)\n",
    "    df['signal_2'] = df['signal'] ** 2\n",
    "    return df\n",
    "\n",
    "# fillna with the mean and select features for training\n",
    "def feature_selection(train, test):\n",
    "    features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)\n",
    "    test = test.replace([np.inf, -np.inf], np.nan)\n",
    "    for feature in features:\n",
    "        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n",
    "        train[feature] = train[feature].fillna(feature_mean)\n",
    "        test[feature] = test[feature].fillna(feature_mean)\n",
    "    return train, test, features\n",
    "\n",
    "#prepare dataset for knn\n",
    "def preproc_for_knn(df):\n",
    "    feats = ['signal_shift_neg_3','signal_shift_neg_2','signal_shift_neg_1','signal',\n",
    "            'signal_shift_pos_1','signal_shift_pos_2', 'signal_shift_pos_3']\n",
    "    \n",
    "    coefs = np.array([0.25, 0.5, 1.0, 4.0, 1.0, 0.5, 0.25])\n",
    "    return df[feats] * coefs\n",
    "    \n",
    "#split knn dataset into hand-crafted groups \n",
    "def get_groups_for_knn(train, test):\n",
    "    train_knn_group = np.full(-1, train.shape[0])\n",
    "    \n",
    "    x = [(0,500000),(1000000,1500000),(1500000,2000000),(2500000,3000000),(2000000,2500000)]\n",
    "    for k in range(5): train_knn_group[x[k][0]:x[k][1]] = k\n",
    "        \n",
    "    test_knn_group = np.full(-1, test.shape[0])\n",
    "    x = [[(0,100000),(300000,400000),(800000,900000),(1000000,2000000)],[(400000,500000)], \n",
    "         [(100000,200000),(900000,1000000)],[(200000,300000),(600000,700000)],[(500000,600000),(700000,800000)]]\n",
    "    \n",
    "    for k in range(5):\n",
    "        for j in range(len(x[k])): test_knn_group[x[k][j][0]:x[k][j][1]] = k\n",
    "    \n",
    "    assert not -1 in train_knn_group\n",
    "    assert not -1 in test_knn_group\n",
    "\n",
    "    return train_knn_group, test_knn_group\n",
    "\n",
    "def knn_feats_train_predict_by_groups(train, test, train_knn_group, test_knn_group):\n",
    "    feats = np.zeros(test.shape[0], NUMBER_OF_KNN_FEATS)\n",
    "    for group in range(5):\n",
    "        model = NearestNeighborsFeats(k_list=[51, 101])\n",
    "        model.fit(train[train_knn_group==group])\n",
    "        feats[test_knn_group==group] = model.predict(test[test_knn_group==group])\n",
    "        \n",
    "    return feats\n",
    "    \n",
    "def run_cv_model_by_batch(train, test, splits, batch_col, feats, sample_submission):\n",
    "    \n",
    "    seed_everything(SEED)\n",
    "    \n",
    "    oof_ = np.zeros((len(train), 11)) # build out of folds matrix with 11 columns, they represent our target variables classes (from 0 to 10)\n",
    "    preds_ = np.zeros((len(test), 11))\n",
    "    \n",
    "    target = ['open_channels']\n",
    "    group = train['group']\n",
    "    \n",
    "    kf = GroupKFold(n_splits=5)\n",
    "    cv = kf.split(train, train[target], group)    \n",
    "    \n",
    "    target = train.open_channels.values\n",
    "    train = preproc_for_knn(train).values\n",
    "    test = preproc_for_knn(test).values\n",
    "    \n",
    "    print(train.shape, test.shape)\n",
    "    \n",
    "    train_knn_group, test_knn_group = get_groups_for_knn(train, test)\n",
    "\n",
    "    oof_knn_feats = np.zeros(train.shape[0], NUMBER_OF_KNN_FEATS)    \n",
    "    \n",
    "    for train_idx, val_idx in cv:\n",
    "        oof_knn_feats[val_idx] = knn_feats_train_predict_by_groups(\n",
    "            train[train_idx], train[val_idx], train_knn_group[train_idx], train_knn_group[val_idx])\n",
    "        \n",
    "    np.save('oof_knn_feats.npy', oof_knn_feats)\n",
    "  \n",
    "\n",
    "    test_knn_feats = knn_feats_train_predict_by_groups(train, test, train_knn_group, test_knn_group)\n",
    "    np.save('test_knn_feats.npy', test_knn_feats)\n",
    "\n",
    "    index_of_median_feature = -6\n",
    "    oof_pred = oof_knn_feats[:, index_of_median_feature]\n",
    "    \n",
    "    test_pred = test_knn_feats[:, index_of_median_feature]\n",
    "    f1_score_ = f1_score(oof_pred, target, average='macro') \n",
    "    \n",
    "    print(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n",
    "    sample_submission['open_channels'] = test_pred.astype(int)\n",
    "    sample_submission.to_csv('submission_knn.csv', index=False, float_format='%.4f')\n",
    "    \n",
    " \n",
    "    \n",
    "# this function run our entire program\n",
    "def run_everything():\n",
    "    \n",
    "    print('Reading Data Started...')\n",
    "    train, test, sample_submission = read_data()\n",
    "    train, test = normalize(train, test)\n",
    "    print('Reading and Normalizing Data Completed')\n",
    "        \n",
    "    print('Creating Features')\n",
    "    print('Feature Engineering Started...')\n",
    "    train = run_feat_engineering(train, batch_size = GROUP_BATCH_SIZE)\n",
    "    test = run_feat_engineering(test, batch_size = GROUP_BATCH_SIZE)\n",
    "    train, test, features = feature_selection(train, test)\n",
    "    print('Feature Engineering Completed...')\n",
    "        \n",
    "   \n",
    "    print(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started...')\n",
    "    run_cv_model_by_batch(train, test, SPLITS, 'group', features, sample_submission)\n",
    "    print('Training completed...')\n",
    "        \n",
    "run_everything()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
