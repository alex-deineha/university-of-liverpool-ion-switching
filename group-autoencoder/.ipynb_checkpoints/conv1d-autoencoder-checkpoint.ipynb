{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks to https://www.kaggle.com/siavrez/wavenet-keras and Sergey Bryansky.\n",
    "# You can take a look at Sergey's kernel [here](https://www.kaggle.com/sggpls/shifted-rfc-pipeline) or [here](https://www.kaggle.com/sggpls/wavenet-with-shifted-rfc-proba). Also, Sergey's [data is here.](https://www.kaggle.com/sggpls/ion-shifted-rfc-proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in /opt/conda/lib/python3.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow_addons) (2.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import losses, models, optimizers\n",
    "import tensorflow_addons as tfa\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations and main hyperparammeters\n",
    "EPOCHS = 180\n",
    "NNBATCHSIZE = 16\n",
    "GROUP_BATCH_SIZE = 4000\n",
    "SEED = 321\n",
    "LR = 0.001\n",
    "SPLITS = 5\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4000, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 2000, 64)          1088      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1000, 128)         65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 256)          262400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 500, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 250, 512)          524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 250, 512)          2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 250, 512)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 125, 512)          1049088   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 125, 512)          2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 125, 512)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               32768512  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64000)             32832000  \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 1, 125, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 1, 250, 512)       1049088   \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 1, 500, 256)       524544    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 1, 1000, 128)      262272    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 1, 2000, 64)       65600     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 1, 4000, 1)        1025      \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 4000, 1)           0         \n",
      "=================================================================\n",
      "Total params: 69,552,033\n",
      "Trainable params: 69,549,089\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def Classifier(shape_):\n",
    "    \n",
    "    def cbr(x, out_layer, kernel, stride, dilation):\n",
    "        x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    inp = Input(shape = (shape_))\n",
    "\n",
    "    x = cbr(inp, 64, 16, 2, 1)\n",
    "    x = cbr(x, 128, 8, 2, 1)\n",
    "    \n",
    "    x = cbr(x, 256, 8, 2, 1)\n",
    "    x = cbr(x, 512, 4, 2, 1)\n",
    "    \n",
    "    x = cbr(x, 512, 4, 2, 1)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = Dense(128)(x)\n",
    "    \n",
    "    enc = Dense(32)(x)\n",
    "    \n",
    "    x = Dense(128)(enc)\n",
    "    x = Dense(512)(x)\n",
    "    x = Dense(125 * 512)(x)\n",
    "    x = Reshape((1, 125,512))(x)\n",
    "    \n",
    "    x = Conv2DTranspose(filters=512, strides=(1,2), kernel_size=(1, 4), padding='same')(x)\n",
    "\n",
    "    \n",
    "    x = Conv2DTranspose(filters=256, strides=(1,2), kernel_size=(1, 4), padding='same')(x)\n",
    "    x = Conv2DTranspose(filters=128, strides=(1,2), kernel_size=(1,8), padding='same')(x)\n",
    "    \n",
    "    x = Conv2DTranspose(filters=64, strides=(1,2), kernel_size=(1, 8), padding='same')(x)\n",
    "    x = Conv2DTranspose(filters=1,strides=(1,2), kernel_size=(1,16), padding='same')(x)\n",
    "\n",
    "    autoenc = x = Reshape((4000,1))(x)\n",
    "    \n",
    "    \n",
    "    encoder = models.Model(inputs = inp, outputs = enc)\n",
    "    \n",
    "    autoencoder = models.Model(inputs = inp, outputs = autoenc)\n",
    "    opt = tfa.optimizers.SWA(Adam(lr = LR))\n",
    "    autoencoder.compile(loss = 'mse', optimizer = opt)\n",
    "    \n",
    "    return autoencoder, encoder\n",
    "\n",
    "Classifier((4000, 1))[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " def previous_lr_schedule(epoch):\n",
    "    if epoch < 30:\n",
    "        lr = LR\n",
    "    elif epoch < 40:\n",
    "        lr = LR / 3\n",
    "    elif epoch < 50:\n",
    "        lr = LR / 5\n",
    "    elif epoch < 60:\n",
    "        lr = LR / 7\n",
    "    elif epoch < 70:\n",
    "        lr = LR / 9\n",
    "    elif epoch < 80:\n",
    "        lr = LR / 11\n",
    "    elif epoch < 90:\n",
    "        lr = LR / 13\n",
    "    else:\n",
    "        lr = LR / 100\n",
    "    return lr\n",
    "\n",
    "def base_lr_schedule(epoch):\n",
    "    return LR*0.956**epoch\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    base_lr = base_lr_schedule(epoch)\n",
    "    max_lr = 0.002\n",
    "    step_size = 2\n",
    "    gamma = 0.965\n",
    "    cycle = np.floor(1+epoch/(2*step_size))\n",
    "    x = np.abs(epoch/step_size - 2*cycle + 1)\n",
    "    lr = base_lr + (max_lr-base_lr)*np.maximum(0, (1-x))*gamma**(epoch)\n",
    "    return lr\n",
    "\n",
    "\n",
    "plt.plot([lr_schedule(i) for i in range(180)])\n",
    "plt.plot([previous_lr_schedule(i) for i in range(180)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data Started...\n",
      "Reading and Normalizing Data Completed\n",
      "Creating Features\n",
      "Feature Engineering Started...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>signal</th>\n",
       "      <th>open_channels</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>-1.146217</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-1.184806</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0003</td>\n",
       "      <td>-1.004040</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0004</td>\n",
       "      <td>-1.299603</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>-1.304482</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     time    signal  open_channels  group\n",
       "0  0.0001 -1.146217              0      0\n",
       "1  0.0002 -1.184806              0      0\n",
       "2  0.0003 -1.004040              0      0\n",
       "3  0.0004 -1.299603              0      0\n",
       "4  0.0005 -1.304482              0      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>signal</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500.000092</td>\n",
       "      <td>-1.101795</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500.000214</td>\n",
       "      <td>-1.182291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500.000305</td>\n",
       "      <td>-1.186577</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500.000397</td>\n",
       "      <td>-1.015220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500.000488</td>\n",
       "      <td>-1.088015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time    signal  group\n",
       "0  500.000092 -1.101795      0\n",
       "1  500.000214 -1.182291      0\n",
       "2  500.000305 -1.186577      0\n",
       "3  500.000397 -1.015220      0\n",
       "4  500.000488 -1.088015      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering Completed...\n",
      "Training Wavenet model with 5 folds of GroupKFold Started...\n",
      "(1250, 4000, 1)\n",
      "Our training dataset shape is (1000, 4000, 1)\n",
      "Our validation dataset shape is (250, 4000, 1)\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/180\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "1000/1000 - 10s - loss: 30.4796 - val_loss: 0.6653\n",
      "Epoch 2/180\n",
      "1000/1000 - 3s - loss: 0.4758 - val_loss: 0.5309\n",
      "Epoch 3/180\n",
      "1000/1000 - 3s - loss: 0.3085 - val_loss: 0.4671\n",
      "Epoch 4/180\n",
      "1000/1000 - 3s - loss: 0.2761 - val_loss: 0.4578\n",
      "Epoch 5/180\n",
      "1000/1000 - 3s - loss: 0.2537 - val_loss: 0.3736\n",
      "Epoch 6/180\n",
      "1000/1000 - 3s - loss: 0.2596 - val_loss: 0.2810\n",
      "Epoch 7/180\n",
      "1000/1000 - 3s - loss: 0.2710 - val_loss: 0.2329\n",
      "Epoch 8/180\n",
      "1000/1000 - 3s - loss: 0.2533 - val_loss: 0.2210\n",
      "Epoch 9/180\n",
      "1000/1000 - 3s - loss: 0.2333 - val_loss: 0.2332\n",
      "Epoch 10/180\n",
      "1000/1000 - 3s - loss: 0.2676 - val_loss: 0.2558\n",
      "Epoch 11/180\n",
      "1000/1000 - 3s - loss: 0.2450 - val_loss: 0.2398\n",
      "Epoch 12/180\n",
      "1000/1000 - 3s - loss: 0.2412 - val_loss: 0.2226\n",
      "Epoch 13/180\n",
      "1000/1000 - 3s - loss: 0.2398 - val_loss: 0.2246\n",
      "Epoch 14/180\n",
      "1000/1000 - 3s - loss: 0.2268 - val_loss: 0.2478\n",
      "Epoch 15/180\n",
      "1000/1000 - 3s - loss: 0.2625 - val_loss: 0.2335\n",
      "Epoch 16/180\n",
      "1000/1000 - 3s - loss: 0.2197 - val_loss: 0.2194\n",
      "Epoch 17/180\n",
      "1000/1000 - 3s - loss: 0.2224 - val_loss: 0.2181\n",
      "Epoch 18/180\n",
      "1000/1000 - 3s - loss: 0.2146 - val_loss: 0.2137\n",
      "Epoch 19/180\n",
      "1000/1000 - 3s - loss: 0.2425 - val_loss: 0.2347\n",
      "Epoch 20/180\n",
      "1000/1000 - 3s - loss: 0.2513 - val_loss: 0.2309\n",
      "Epoch 21/180\n",
      "1000/1000 - 3s - loss: 0.2159 - val_loss: 0.2104\n",
      "Epoch 22/180\n",
      "1000/1000 - 3s - loss: 0.2380 - val_loss: 0.2372\n",
      "Epoch 23/180\n",
      "1000/1000 - 3s - loss: 0.2270 - val_loss: 0.2346\n",
      "Epoch 24/180\n",
      "1000/1000 - 3s - loss: 0.2060 - val_loss: 0.2118\n",
      "Epoch 25/180\n",
      "1000/1000 - 3s - loss: 0.2083 - val_loss: 0.2082\n",
      "Epoch 26/180\n",
      "1000/1000 - 3s - loss: 0.2084 - val_loss: 0.2376\n",
      "Epoch 27/180\n",
      "1000/1000 - 3s - loss: 0.2039 - val_loss: 0.2336\n",
      "Epoch 28/180\n",
      "1000/1000 - 3s - loss: 0.2071 - val_loss: 0.2209\n",
      "Epoch 29/180\n",
      "1000/1000 - 3s - loss: 0.2038 - val_loss: 0.2100\n",
      "Epoch 30/180\n",
      "1000/1000 - 3s - loss: 0.1946 - val_loss: 0.2176\n",
      "Epoch 31/180\n",
      "1000/1000 - 3s - loss: 0.2267 - val_loss: 0.2211\n",
      "Epoch 32/180\n",
      "1000/1000 - 3s - loss: 0.1943 - val_loss: 0.2059\n",
      "Epoch 33/180\n",
      "1000/1000 - 3s - loss: 0.1900 - val_loss: 0.2014\n",
      "Epoch 34/180\n",
      "1000/1000 - 3s - loss: 0.1942 - val_loss: 0.2119\n",
      "Epoch 35/180\n",
      "1000/1000 - 3s - loss: 0.1983 - val_loss: 0.2000\n",
      "Epoch 36/180\n",
      "1000/1000 - 3s - loss: 0.1919 - val_loss: 0.2100\n",
      "Epoch 37/180\n",
      "1000/1000 - 3s - loss: 0.1903 - val_loss: 0.2005\n",
      "Epoch 38/180\n",
      "1000/1000 - 3s - loss: 0.1972 - val_loss: 0.2389\n",
      "Epoch 39/180\n",
      "1000/1000 - 3s - loss: 0.1967 - val_loss: 0.2120\n",
      "Epoch 40/180\n",
      "1000/1000 - 3s - loss: 0.1852 - val_loss: 0.1966\n",
      "Epoch 41/180\n",
      "1000/1000 - 3s - loss: 0.1840 - val_loss: 0.1944\n",
      "Epoch 42/180\n",
      "1000/1000 - 3s - loss: 0.1863 - val_loss: 0.1960\n",
      "Epoch 43/180\n",
      "1000/1000 - 3s - loss: 0.1926 - val_loss: 0.2060\n",
      "Epoch 44/180\n",
      "1000/1000 - 3s - loss: 0.1766 - val_loss: 0.1965\n",
      "Epoch 45/180\n",
      "1000/1000 - 3s - loss: 0.1829 - val_loss: 0.2035\n",
      "Epoch 46/180\n",
      "1000/1000 - 3s - loss: 0.1883 - val_loss: 0.2003\n",
      "Epoch 47/180\n",
      "1000/1000 - 3s - loss: 0.2070 - val_loss: 0.2003\n",
      "Epoch 48/180\n",
      "1000/1000 - 3s - loss: 0.1927 - val_loss: 0.2019\n",
      "Epoch 49/180\n",
      "1000/1000 - 3s - loss: 0.1817 - val_loss: 0.1920\n",
      "Epoch 50/180\n",
      "1000/1000 - 3s - loss: 0.1895 - val_loss: 0.1941\n",
      "Epoch 51/180\n",
      "1000/1000 - 3s - loss: 0.1824 - val_loss: 0.2247\n",
      "Epoch 52/180\n",
      "1000/1000 - 3s - loss: 0.1798 - val_loss: 0.1972\n",
      "Epoch 53/180\n",
      "1000/1000 - 3s - loss: 0.1777 - val_loss: 0.1968\n",
      "Epoch 54/180\n",
      "1000/1000 - 3s - loss: 0.1888 - val_loss: 0.2129\n",
      "Epoch 55/180\n",
      "1000/1000 - 3s - loss: 0.1853 - val_loss: 0.2100\n",
      "Epoch 56/180\n",
      "1000/1000 - 3s - loss: 0.1938 - val_loss: 0.1917\n",
      "Epoch 57/180\n",
      "1000/1000 - 3s - loss: 0.1722 - val_loss: 0.1909\n",
      "Epoch 58/180\n",
      "1000/1000 - 3s - loss: 0.1686 - val_loss: 0.1895\n",
      "Epoch 59/180\n",
      "1000/1000 - 3s - loss: 0.1878 - val_loss: 0.2071\n",
      "Epoch 60/180\n",
      "1000/1000 - 3s - loss: 0.1788 - val_loss: 0.1918\n",
      "Epoch 61/180\n",
      "1000/1000 - 3s - loss: 0.1723 - val_loss: 0.1918\n",
      "Epoch 62/180\n",
      "1000/1000 - 3s - loss: 0.1726 - val_loss: 0.1938\n",
      "Epoch 63/180\n",
      "1000/1000 - 3s - loss: 0.1784 - val_loss: 0.1902\n",
      "Epoch 64/180\n",
      "1000/1000 - 3s - loss: 0.1752 - val_loss: 0.1882\n",
      "Epoch 65/180\n",
      "1000/1000 - 3s - loss: 0.1701 - val_loss: 0.1866\n",
      "Epoch 66/180\n",
      "1000/1000 - 3s - loss: 0.1819 - val_loss: 0.1966\n",
      "Epoch 67/180\n",
      "1000/1000 - 3s - loss: 0.1743 - val_loss: 0.1889\n",
      "Epoch 68/180\n",
      "1000/1000 - 3s - loss: 0.1666 - val_loss: 0.1870\n",
      "Epoch 69/180\n",
      "1000/1000 - 3s - loss: 0.1754 - val_loss: 0.1922\n",
      "Epoch 70/180\n",
      "1000/1000 - 3s - loss: 0.1842 - val_loss: 0.1873\n",
      "Epoch 71/180\n",
      "1000/1000 - 3s - loss: 0.1894 - val_loss: 0.2400\n",
      "Epoch 72/180\n",
      "1000/1000 - 3s - loss: 0.1797 - val_loss: 0.1859\n",
      "Epoch 73/180\n",
      "1000/1000 - 3s - loss: 0.1694 - val_loss: 0.1859\n",
      "Epoch 74/180\n",
      "1000/1000 - 3s - loss: 0.1714 - val_loss: 0.1892\n",
      "Epoch 75/180\n",
      "1000/1000 - 3s - loss: 0.1725 - val_loss: 0.1860\n",
      "Epoch 76/180\n",
      "1000/1000 - 3s - loss: 0.1699 - val_loss: 0.1944\n",
      "Epoch 77/180\n",
      "1000/1000 - 3s - loss: 0.1662 - val_loss: 0.1858\n",
      "Epoch 78/180\n",
      "1000/1000 - 3s - loss: 0.1736 - val_loss: 0.1919\n",
      "Epoch 79/180\n",
      "1000/1000 - 3s - loss: 0.1674 - val_loss: 0.1859\n",
      "Epoch 80/180\n",
      "1000/1000 - 3s - loss: 0.1651 - val_loss: 0.1857\n",
      "Epoch 81/180\n",
      "1000/1000 - 3s - loss: 0.1690 - val_loss: 0.1834\n",
      "Epoch 82/180\n",
      "1000/1000 - 3s - loss: 0.1657 - val_loss: 0.1862\n",
      "Epoch 83/180\n",
      "1000/1000 - 3s - loss: 0.1730 - val_loss: 0.1856\n",
      "Epoch 84/180\n",
      "1000/1000 - 3s - loss: 0.1650 - val_loss: 0.1915\n",
      "Epoch 85/180\n",
      "1000/1000 - 3s - loss: 0.1638 - val_loss: 0.1858\n",
      "Epoch 86/180\n",
      "1000/1000 - 3s - loss: 0.1704 - val_loss: 0.1831\n",
      "Epoch 87/180\n",
      "1000/1000 - 3s - loss: 0.1672 - val_loss: 0.1872\n",
      "Epoch 88/180\n",
      "1000/1000 - 3s - loss: 0.1658 - val_loss: 0.2076\n",
      "Epoch 89/180\n",
      "1000/1000 - 3s - loss: 0.1623 - val_loss: 0.1846\n",
      "Epoch 90/180\n",
      "1000/1000 - 3s - loss: 0.1594 - val_loss: 0.1845\n",
      "Epoch 91/180\n",
      "1000/1000 - 3s - loss: 0.1636 - val_loss: 0.1822\n",
      "Epoch 92/180\n",
      "1000/1000 - 3s - loss: 0.1679 - val_loss: 0.1808\n",
      "Epoch 93/180\n",
      "1000/1000 - 3s - loss: 0.1583 - val_loss: 0.1810\n",
      "Epoch 94/180\n",
      "1000/1000 - 3s - loss: 0.1604 - val_loss: 0.1820\n",
      "Epoch 95/180\n",
      "1000/1000 - 3s - loss: 0.1661 - val_loss: 0.1853\n",
      "Epoch 96/180\n",
      "1000/1000 - 3s - loss: 0.1614 - val_loss: 0.2259\n",
      "Epoch 97/180\n",
      "1000/1000 - 3s - loss: 0.1649 - val_loss: 0.1823\n",
      "Epoch 98/180\n",
      "1000/1000 - 3s - loss: 0.1685 - val_loss: 0.1816\n",
      "Epoch 99/180\n",
      "1000/1000 - 3s - loss: 0.1623 - val_loss: 0.1828\n",
      "Epoch 100/180\n",
      "1000/1000 - 3s - loss: 0.1613 - val_loss: 0.1992\n",
      "Epoch 101/180\n",
      "1000/1000 - 3s - loss: 0.1598 - val_loss: 0.1818\n",
      "Epoch 102/180\n",
      "1000/1000 - 3s - loss: 0.1552 - val_loss: 0.1816\n",
      "Epoch 103/180\n",
      "1000/1000 - 3s - loss: 0.1719 - val_loss: 0.1851\n",
      "Epoch 104/180\n",
      "1000/1000 - 3s - loss: 0.1586 - val_loss: 0.1878\n",
      "Epoch 105/180\n",
      "1000/1000 - 3s - loss: 0.1613 - val_loss: 0.1820\n",
      "Epoch 106/180\n",
      "1000/1000 - 3s - loss: 0.1618 - val_loss: 0.1809\n",
      "Epoch 107/180\n",
      "1000/1000 - 3s - loss: 0.1586 - val_loss: 0.1821\n",
      "Epoch 108/180\n",
      "1000/1000 - 3s - loss: 0.1568 - val_loss: 0.1851\n",
      "Epoch 109/180\n",
      "1000/1000 - 3s - loss: 0.1569 - val_loss: 0.1817\n",
      "Epoch 110/180\n",
      "1000/1000 - 3s - loss: 0.1583 - val_loss: 0.1798\n",
      "Epoch 111/180\n",
      "1000/1000 - 3s - loss: 0.1609 - val_loss: 0.1851\n",
      "Epoch 112/180\n",
      "1000/1000 - 3s - loss: 0.1639 - val_loss: 0.1858\n",
      "Epoch 113/180\n",
      "1000/1000 - 3s - loss: 0.1663 - val_loss: 0.1852\n",
      "Epoch 114/180\n",
      "1000/1000 - 3s - loss: 0.1578 - val_loss: 0.1837\n",
      "Epoch 115/180\n",
      "1000/1000 - 3s - loss: 0.1640 - val_loss: 0.1831\n",
      "Epoch 116/180\n",
      "1000/1000 - 3s - loss: 0.1579 - val_loss: 0.1825\n",
      "Epoch 117/180\n",
      "1000/1000 - 3s - loss: 0.1584 - val_loss: 0.1804\n",
      "Epoch 118/180\n",
      "1000/1000 - 3s - loss: 0.1558 - val_loss: 0.1825\n",
      "Epoch 119/180\n",
      "1000/1000 - 3s - loss: 0.1551 - val_loss: 0.1806\n",
      "Epoch 120/180\n",
      "1000/1000 - 3s - loss: 0.1573 - val_loss: 0.1802\n",
      "Epoch 121/180\n",
      "1000/1000 - 3s - loss: 0.1598 - val_loss: 0.1810\n",
      "Epoch 122/180\n",
      "1000/1000 - 3s - loss: 0.1638 - val_loss: 0.1992\n",
      "Epoch 123/180\n",
      "1000/1000 - 3s - loss: 0.1659 - val_loss: 0.1854\n",
      "Epoch 124/180\n",
      "1000/1000 - 3s - loss: 0.1578 - val_loss: 0.1820\n",
      "Epoch 125/180\n",
      "1000/1000 - 3s - loss: 0.1548 - val_loss: 0.1812\n",
      "Epoch 126/180\n",
      "1000/1000 - 3s - loss: 0.1661 - val_loss: 0.1821\n",
      "Epoch 127/180\n",
      "1000/1000 - 3s - loss: 0.1622 - val_loss: 0.1851\n",
      "Epoch 128/180\n",
      "1000/1000 - 3s - loss: 0.1563 - val_loss: 0.1819\n",
      "Epoch 129/180\n",
      "1000/1000 - 3s - loss: 0.1547 - val_loss: 0.1818\n",
      "Epoch 130/180\n",
      "Restoring model weights from the end of the best epoch.\n",
      "1000/1000 - 3s - loss: 0.1576 - val_loss: 0.1809\n",
      "Epoch 00130: early stopping\n",
      "INFO:tensorflow:Assets written to: model_0/assets\n",
      "INFO:tensorflow:Assets written to: encoder_0/assets\n",
      "Our training dataset shape is (1000, 4000, 1)\n",
      "Our validation dataset shape is (250, 4000, 1)\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/180\n",
      "1000/1000 - 8s - loss: 356.3033 - val_loss: 9.1044\n",
      "Epoch 2/180\n",
      "1000/1000 - 3s - loss: 1.5816 - val_loss: 0.7100\n",
      "Epoch 3/180\n",
      "1000/1000 - 3s - loss: 0.4526 - val_loss: 0.6163\n",
      "Epoch 4/180\n",
      "1000/1000 - 3s - loss: 0.3711 - val_loss: 0.4754\n",
      "Epoch 5/180\n",
      "1000/1000 - 3s - loss: 0.3226 - val_loss: 0.3793\n",
      "Epoch 6/180\n",
      "1000/1000 - 3s - loss: 0.3514 - val_loss: 0.3978\n",
      "Epoch 7/180\n",
      "1000/1000 - 3s - loss: 0.3483 - val_loss: 0.2902\n",
      "Epoch 8/180\n",
      "1000/1000 - 3s - loss: 0.3344 - val_loss: 0.2462\n",
      "Epoch 9/180\n",
      "1000/1000 - 3s - loss: 0.2983 - val_loss: 0.2434\n",
      "Epoch 10/180\n",
      "1000/1000 - 3s - loss: 0.3100 - val_loss: 0.2212\n",
      "Epoch 11/180\n",
      "1000/1000 - 3s - loss: 0.3155 - val_loss: 0.2329\n",
      "Epoch 12/180\n",
      "1000/1000 - 3s - loss: 0.3019 - val_loss: 0.2389\n",
      "Epoch 13/180\n",
      "1000/1000 - 3s - loss: 0.2964 - val_loss: 0.2260\n",
      "Epoch 14/180\n",
      "1000/1000 - 3s - loss: 0.2861 - val_loss: 0.2606\n",
      "Epoch 15/180\n",
      "1000/1000 - 3s - loss: 0.2956 - val_loss: 0.2323\n",
      "Epoch 16/180\n",
      "1000/1000 - 3s - loss: 0.2611 - val_loss: 0.2209\n",
      "Epoch 17/180\n",
      "1000/1000 - 3s - loss: 0.2719 - val_loss: 0.2114\n",
      "Epoch 18/180\n",
      "1000/1000 - 3s - loss: 0.2546 - val_loss: 0.2166\n",
      "Epoch 19/180\n",
      "1000/1000 - 3s - loss: 0.3003 - val_loss: 0.2264\n",
      "Epoch 20/180\n",
      "1000/1000 - 3s - loss: 0.2814 - val_loss: 0.2139\n",
      "Epoch 21/180\n",
      "1000/1000 - 3s - loss: 0.2493 - val_loss: 0.2151\n",
      "Epoch 22/180\n",
      "1000/1000 - 3s - loss: 0.2838 - val_loss: 0.2179\n",
      "Epoch 23/180\n",
      "1000/1000 - 3s - loss: 0.2752 - val_loss: 0.2341\n",
      "Epoch 24/180\n",
      "1000/1000 - 3s - loss: 0.2485 - val_loss: 0.2183\n",
      "Epoch 25/180\n",
      "1000/1000 - 3s - loss: 0.2530 - val_loss: 0.2101\n",
      "Epoch 26/180\n",
      "1000/1000 - 3s - loss: 0.2486 - val_loss: 0.2229\n",
      "Epoch 27/180\n",
      "1000/1000 - 3s - loss: 0.2444 - val_loss: 0.2133\n",
      "Epoch 28/180\n",
      "1000/1000 - 3s - loss: 0.2472 - val_loss: 0.2124\n",
      "Epoch 29/180\n",
      "1000/1000 - 3s - loss: 0.2478 - val_loss: 0.2150\n",
      "Epoch 30/180\n",
      "1000/1000 - 3s - loss: 0.2334 - val_loss: 0.2660\n",
      "Epoch 31/180\n",
      "1000/1000 - 3s - loss: 0.2661 - val_loss: 0.2134\n",
      "Epoch 32/180\n",
      "1000/1000 - 3s - loss: 0.2325 - val_loss: 0.2101\n",
      "Epoch 33/180\n",
      "1000/1000 - 3s - loss: 0.2361 - val_loss: 0.2149\n",
      "Epoch 34/180\n",
      "1000/1000 - 3s - loss: 0.2337 - val_loss: 0.2091\n",
      "Epoch 35/180\n",
      "1000/1000 - 3s - loss: 0.2378 - val_loss: 0.2189\n",
      "Epoch 36/180\n",
      "1000/1000 - 3s - loss: 0.2334 - val_loss: 0.2373\n",
      "Epoch 37/180\n",
      "1000/1000 - 3s - loss: 0.2358 - val_loss: 0.2125\n",
      "Epoch 38/180\n",
      "1000/1000 - 3s - loss: 0.2335 - val_loss: 0.2232\n",
      "Epoch 39/180\n",
      "1000/1000 - 3s - loss: 0.2312 - val_loss: 0.2148\n",
      "Epoch 40/180\n",
      "1000/1000 - 3s - loss: 0.2268 - val_loss: 0.2070\n",
      "Epoch 41/180\n",
      "1000/1000 - 3s - loss: 0.2277 - val_loss: 0.2091\n",
      "Epoch 42/180\n",
      "1000/1000 - 3s - loss: 0.2335 - val_loss: 0.2196\n",
      "Epoch 43/180\n",
      "1000/1000 - 3s - loss: 0.2418 - val_loss: 0.2104\n",
      "Epoch 44/180\n",
      "1000/1000 - 3s - loss: 0.2203 - val_loss: 0.2145\n",
      "Epoch 45/180\n",
      "1000/1000 - 3s - loss: 0.2355 - val_loss: 0.2270\n",
      "Epoch 46/180\n",
      "1000/1000 - 3s - loss: 0.2398 - val_loss: 0.2301\n",
      "Epoch 47/180\n",
      "1000/1000 - 3s - loss: 0.2478 - val_loss: 0.2390\n",
      "Epoch 48/180\n",
      "1000/1000 - 3s - loss: 0.2318 - val_loss: 0.2278\n",
      "Epoch 49/180\n",
      "1000/1000 - 3s - loss: 0.2287 - val_loss: 0.2064\n",
      "Epoch 50/180\n",
      "1000/1000 - 3s - loss: 0.2306 - val_loss: 0.2084\n",
      "Epoch 51/180\n",
      "1000/1000 - 3s - loss: 0.2341 - val_loss: 0.2071\n",
      "Epoch 52/180\n",
      "1000/1000 - 3s - loss: 0.2255 - val_loss: 0.2074\n",
      "Epoch 53/180\n",
      "1000/1000 - 3s - loss: 0.2239 - val_loss: 0.2063\n",
      "Epoch 54/180\n",
      "1000/1000 - 3s - loss: 0.2362 - val_loss: 0.2106\n",
      "Epoch 55/180\n",
      "1000/1000 - 3s - loss: 0.2234 - val_loss: 0.2116\n",
      "Epoch 56/180\n",
      "1000/1000 - 3s - loss: 0.2329 - val_loss: 0.2087\n",
      "Epoch 57/180\n",
      "1000/1000 - 3s - loss: 0.2198 - val_loss: 0.2082\n",
      "Epoch 58/180\n",
      "1000/1000 - 3s - loss: 0.2153 - val_loss: 0.2055\n",
      "Epoch 59/180\n",
      "1000/1000 - 3s - loss: 0.2238 - val_loss: 0.2096\n",
      "Epoch 60/180\n",
      "1000/1000 - 3s - loss: 0.2235 - val_loss: 0.2043\n",
      "Epoch 61/180\n",
      "1000/1000 - 3s - loss: 0.2190 - val_loss: 0.2059\n",
      "Epoch 62/180\n",
      "1000/1000 - 3s - loss: 0.2173 - val_loss: 0.2087\n",
      "Epoch 63/180\n",
      "1000/1000 - 3s - loss: 0.2257 - val_loss: 0.2072\n",
      "Epoch 64/180\n",
      "1000/1000 - 3s - loss: 0.2343 - val_loss: 0.2035\n",
      "Epoch 65/180\n",
      "1000/1000 - 3s - loss: 0.2173 - val_loss: 0.2032\n",
      "Epoch 66/180\n",
      "1000/1000 - 3s - loss: 0.2282 - val_loss: 0.2047\n",
      "Epoch 67/180\n",
      "1000/1000 - 3s - loss: 0.2161 - val_loss: 0.2113\n",
      "Epoch 68/180\n",
      "1000/1000 - 3s - loss: 0.2142 - val_loss: 0.2066\n",
      "Epoch 69/180\n",
      "1000/1000 - 3s - loss: 0.2234 - val_loss: 0.2069\n",
      "Epoch 70/180\n",
      "1000/1000 - 3s - loss: 0.2254 - val_loss: 0.2031\n",
      "Epoch 71/180\n",
      "1000/1000 - 3s - loss: 0.2329 - val_loss: 0.2429\n",
      "Epoch 72/180\n",
      "1000/1000 - 3s - loss: 0.2241 - val_loss: 0.2070\n",
      "Epoch 73/180\n",
      "1000/1000 - 3s - loss: 0.2187 - val_loss: 0.2030\n",
      "Epoch 74/180\n",
      "1000/1000 - 3s - loss: 0.2168 - val_loss: 0.2064\n",
      "Epoch 75/180\n",
      "1000/1000 - 3s - loss: 0.2201 - val_loss: 0.2031\n",
      "Epoch 76/180\n",
      "1000/1000 - 3s - loss: 0.2202 - val_loss: 0.2064\n",
      "Epoch 77/180\n",
      "1000/1000 - 3s - loss: 0.2154 - val_loss: 0.2022\n",
      "Epoch 78/180\n",
      "1000/1000 - 3s - loss: 0.2270 - val_loss: 0.2059\n",
      "Epoch 79/180\n",
      "1000/1000 - 3s - loss: 0.2122 - val_loss: 0.2068\n",
      "Epoch 80/180\n",
      "1000/1000 - 3s - loss: 0.2135 - val_loss: 0.2017\n",
      "Epoch 81/180\n",
      "1000/1000 - 3s - loss: 0.2193 - val_loss: 0.2048\n",
      "Epoch 82/180\n",
      "1000/1000 - 3s - loss: 0.2132 - val_loss: 0.2055\n",
      "Epoch 83/180\n",
      "1000/1000 - 3s - loss: 0.2225 - val_loss: 0.2191\n",
      "Epoch 84/180\n",
      "1000/1000 - 3s - loss: 0.2143 - val_loss: 0.2160\n",
      "Epoch 85/180\n",
      "1000/1000 - 3s - loss: 0.2151 - val_loss: 0.2032\n",
      "Epoch 86/180\n",
      "1000/1000 - 3s - loss: 0.2182 - val_loss: 0.2024\n",
      "Epoch 87/180\n",
      "1000/1000 - 3s - loss: 0.2160 - val_loss: 0.2058\n",
      "Epoch 88/180\n",
      "1000/1000 - 3s - loss: 0.2149 - val_loss: 0.2068\n",
      "Epoch 89/180\n",
      "1000/1000 - 3s - loss: 0.2075 - val_loss: 0.2041\n",
      "Epoch 90/180\n",
      "1000/1000 - 3s - loss: 0.2082 - val_loss: 0.2022\n",
      "Epoch 91/180\n",
      "1000/1000 - 3s - loss: 0.2143 - val_loss: 0.2053\n",
      "Epoch 92/180\n",
      "1000/1000 - 3s - loss: 0.2113 - val_loss: 0.2031\n",
      "Epoch 93/180\n",
      "1000/1000 - 3s - loss: 0.2038 - val_loss: 0.2019\n",
      "Epoch 94/180\n",
      "1000/1000 - 3s - loss: 0.2082 - val_loss: 0.2040\n",
      "Epoch 95/180\n",
      "1000/1000 - 3s - loss: 0.2243 - val_loss: 0.2001\n",
      "Epoch 96/180\n",
      "1000/1000 - 3s - loss: 0.2116 - val_loss: 0.2250\n",
      "Epoch 97/180\n",
      "1000/1000 - 3s - loss: 0.2148 - val_loss: 0.2022\n",
      "Epoch 98/180\n",
      "1000/1000 - 3s - loss: 0.2162 - val_loss: 0.2012\n",
      "Epoch 99/180\n",
      "1000/1000 - 3s - loss: 0.2094 - val_loss: 0.1997\n",
      "Epoch 100/180\n",
      "1000/1000 - 3s - loss: 0.2065 - val_loss: 0.2054\n",
      "Epoch 101/180\n",
      "1000/1000 - 3s - loss: 0.2081 - val_loss: 0.2005\n",
      "Epoch 102/180\n",
      "1000/1000 - 3s - loss: 0.2025 - val_loss: 0.2023\n",
      "Epoch 103/180\n",
      "1000/1000 - 3s - loss: 0.2101 - val_loss: 0.2019\n",
      "Epoch 104/180\n",
      "1000/1000 - 3s - loss: 0.2046 - val_loss: 0.2089\n",
      "Epoch 105/180\n",
      "1000/1000 - 3s - loss: 0.2084 - val_loss: 0.2030\n",
      "Epoch 106/180\n",
      "1000/1000 - 3s - loss: 0.2094 - val_loss: 0.2019\n",
      "Epoch 107/180\n",
      "1000/1000 - 3s - loss: 0.2056 - val_loss: 0.2025\n",
      "Epoch 108/180\n",
      "1000/1000 - 3s - loss: 0.2024 - val_loss: 0.2036\n",
      "Epoch 109/180\n",
      "1000/1000 - 3s - loss: 0.2058 - val_loss: 0.2015\n",
      "Epoch 110/180\n",
      "1000/1000 - 3s - loss: 0.2071 - val_loss: 0.2011\n",
      "Epoch 111/180\n",
      "1000/1000 - 3s - loss: 0.2099 - val_loss: 0.2053\n",
      "Epoch 112/180\n",
      "1000/1000 - 3s - loss: 0.2122 - val_loss: 0.2147\n",
      "Epoch 113/180\n",
      "1000/1000 - 3s - loss: 0.2134 - val_loss: 0.2023\n",
      "Epoch 114/180\n",
      "1000/1000 - 3s - loss: 0.2065 - val_loss: 0.2039\n",
      "Epoch 115/180\n",
      "1000/1000 - 3s - loss: 0.2097 - val_loss: 0.2087\n",
      "Epoch 116/180\n",
      "1000/1000 - 3s - loss: 0.2067 - val_loss: 0.2007\n",
      "Epoch 117/180\n",
      "1000/1000 - 3s - loss: 0.2067 - val_loss: 0.2015\n",
      "Epoch 118/180\n",
      "1000/1000 - 3s - loss: 0.2040 - val_loss: 0.2044\n",
      "Epoch 119/180\n",
      "Restoring model weights from the end of the best epoch.\n",
      "1000/1000 - 3s - loss: 0.2041 - val_loss: 0.2084\n",
      "Epoch 00119: early stopping\n",
      "INFO:tensorflow:Assets written to: model_1/assets\n",
      "INFO:tensorflow:Assets written to: encoder_1/assets\n",
      "Our training dataset shape is (1000, 4000, 1)\n",
      "Our validation dataset shape is (250, 4000, 1)\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/180\n",
      "1000/1000 - 8s - loss: 132.2676 - val_loss: 2.2492\n",
      "Epoch 2/180\n",
      "1000/1000 - 3s - loss: 0.7846 - val_loss: 0.7650\n",
      "Epoch 3/180\n",
      "1000/1000 - 3s - loss: 0.3818 - val_loss: 0.5296\n",
      "Epoch 4/180\n",
      "1000/1000 - 3s - loss: 0.3149 - val_loss: 0.4319\n",
      "Epoch 5/180\n",
      "1000/1000 - 3s - loss: 0.2938 - val_loss: 0.3268\n",
      "Epoch 6/180\n",
      "1000/1000 - 3s - loss: 0.2860 - val_loss: 0.2829\n",
      "Epoch 7/180\n",
      "1000/1000 - 3s - loss: 0.2888 - val_loss: 0.2566\n",
      "Epoch 8/180\n",
      "1000/1000 - 3s - loss: 0.2798 - val_loss: 0.2495\n",
      "Epoch 9/180\n",
      "1000/1000 - 3s - loss: 0.2626 - val_loss: 0.2396\n",
      "Epoch 10/180\n",
      "1000/1000 - 3s - loss: 0.2842 - val_loss: 0.2246\n",
      "Epoch 11/180\n",
      "1000/1000 - 3s - loss: 0.2813 - val_loss: 0.2377\n",
      "Epoch 12/180\n",
      "1000/1000 - 3s - loss: 0.2757 - val_loss: 0.2223\n",
      "Epoch 13/180\n",
      "1000/1000 - 3s - loss: 0.2618 - val_loss: 0.2251\n",
      "Epoch 14/180\n",
      "1000/1000 - 3s - loss: 0.2576 - val_loss: 0.2310\n",
      "Epoch 15/180\n",
      "1000/1000 - 3s - loss: 0.2756 - val_loss: 0.2231\n",
      "Epoch 16/180\n",
      "1000/1000 - 3s - loss: 0.2440 - val_loss: 0.2147\n",
      "Epoch 17/180\n",
      "1000/1000 - 3s - loss: 0.2520 - val_loss: 0.2148\n",
      "Epoch 18/180\n",
      "1000/1000 - 3s - loss: 0.2443 - val_loss: 0.2196\n",
      "Epoch 19/180\n",
      "1000/1000 - 3s - loss: 0.2624 - val_loss: 0.2218\n",
      "Epoch 20/180\n",
      "1000/1000 - 3s - loss: 0.2649 - val_loss: 0.2265\n",
      "Epoch 21/180\n",
      "1000/1000 - 3s - loss: 0.2393 - val_loss: 0.2200\n",
      "Epoch 22/180\n",
      "1000/1000 - 3s - loss: 0.2670 - val_loss: 0.2531\n",
      "Epoch 23/180\n",
      "1000/1000 - 3s - loss: 0.2490 - val_loss: 0.2351\n",
      "Epoch 24/180\n",
      "1000/1000 - 3s - loss: 0.2388 - val_loss: 0.2185\n",
      "Epoch 25/180\n",
      "1000/1000 - 3s - loss: 0.2480 - val_loss: 0.2130\n",
      "Epoch 26/180\n",
      "1000/1000 - 3s - loss: 0.2413 - val_loss: 0.2330\n",
      "Epoch 27/180\n",
      "1000/1000 - 3s - loss: 0.2395 - val_loss: 0.2198\n",
      "Epoch 28/180\n",
      "1000/1000 - 3s - loss: 0.2421 - val_loss: 0.2185\n",
      "Epoch 29/180\n",
      "1000/1000 - 3s - loss: 0.2329 - val_loss: 0.2292\n",
      "Epoch 30/180\n",
      "1000/1000 - 3s - loss: 0.2321 - val_loss: 0.2414\n",
      "Epoch 31/180\n",
      "1000/1000 - 3s - loss: 0.2594 - val_loss: 0.2147\n",
      "Epoch 32/180\n",
      "1000/1000 - 3s - loss: 0.2299 - val_loss: 0.2155\n",
      "Epoch 33/180\n",
      "1000/1000 - 3s - loss: 0.2312 - val_loss: 0.2175\n",
      "Epoch 34/180\n",
      "1000/1000 - 3s - loss: 0.2358 - val_loss: 0.2155\n",
      "Epoch 35/180\n",
      "1000/1000 - 3s - loss: 0.2377 - val_loss: 0.2184\n",
      "Epoch 36/180\n",
      "1000/1000 - 3s - loss: 0.2316 - val_loss: 0.2225\n",
      "Epoch 37/180\n",
      "1000/1000 - 3s - loss: 0.2285 - val_loss: 0.2139\n",
      "Epoch 38/180\n",
      "1000/1000 - 3s - loss: 0.2332 - val_loss: 0.2233\n",
      "Epoch 39/180\n",
      "1000/1000 - 3s - loss: 0.2306 - val_loss: 0.2162\n",
      "Epoch 40/180\n",
      "1000/1000 - 3s - loss: 0.2246 - val_loss: 0.2124\n",
      "Epoch 41/180\n",
      "1000/1000 - 3s - loss: 0.2244 - val_loss: 0.2141\n",
      "Epoch 42/180\n",
      "1000/1000 - 3s - loss: 0.2336 - val_loss: 0.2110\n",
      "Epoch 43/180\n",
      "1000/1000 - 3s - loss: 0.2468 - val_loss: 0.2265\n",
      "Epoch 44/180\n",
      "1000/1000 - 3s - loss: 0.2188 - val_loss: 0.2100\n",
      "Epoch 45/180\n",
      "1000/1000 - 3s - loss: 0.2295 - val_loss: 0.2180\n",
      "Epoch 46/180\n",
      "1000/1000 - 3s - loss: 0.2331 - val_loss: 0.2114\n",
      "Epoch 47/180\n",
      "1000/1000 - 3s - loss: 0.2376 - val_loss: 0.2287\n",
      "Epoch 48/180\n",
      "1000/1000 - 3s - loss: 0.2301 - val_loss: 0.2190\n",
      "Epoch 49/180\n",
      "1000/1000 - 3s - loss: 0.2201 - val_loss: 0.2093\n",
      "Epoch 50/180\n",
      "1000/1000 - 3s - loss: 0.2242 - val_loss: 0.2120\n",
      "Epoch 51/180\n",
      "1000/1000 - 3s - loss: 0.2260 - val_loss: 0.2207\n",
      "Epoch 52/180\n",
      "1000/1000 - 3s - loss: 0.2252 - val_loss: 0.2164\n",
      "Epoch 53/180\n",
      "1000/1000 - 3s - loss: 0.2204 - val_loss: 0.2091\n",
      "Epoch 54/180\n",
      "1000/1000 - 3s - loss: 0.2315 - val_loss: 0.2121\n",
      "Epoch 55/180\n",
      "1000/1000 - 3s - loss: 0.2222 - val_loss: 0.2129\n",
      "Epoch 56/180\n",
      "1000/1000 - 3s - loss: 0.2279 - val_loss: 0.2155\n",
      "Epoch 57/180\n",
      "1000/1000 - 3s - loss: 0.2145 - val_loss: 0.2075\n",
      "Epoch 58/180\n",
      "1000/1000 - 3s - loss: 0.2095 - val_loss: 0.2062\n",
      "Epoch 59/180\n",
      "1000/1000 - 3s - loss: 0.2242 - val_loss: 0.2205\n",
      "Epoch 60/180\n",
      "1000/1000 - 3s - loss: 0.2203 - val_loss: 0.2074\n",
      "Epoch 61/180\n",
      "1000/1000 - 3s - loss: 0.2138 - val_loss: 0.2099\n",
      "Epoch 62/180\n",
      "1000/1000 - 3s - loss: 0.2135 - val_loss: 0.2066\n",
      "Epoch 63/180\n",
      "1000/1000 - 3s - loss: 0.2154 - val_loss: 0.2120\n",
      "Epoch 64/180\n",
      "1000/1000 - 3s - loss: 0.2252 - val_loss: 0.2087\n",
      "Epoch 65/180\n",
      "1000/1000 - 3s - loss: 0.2102 - val_loss: 0.2047\n",
      "Epoch 66/180\n",
      "1000/1000 - 3s - loss: 0.2208 - val_loss: 0.2083\n",
      "Epoch 67/180\n",
      "1000/1000 - 3s - loss: 0.2123 - val_loss: 0.2263\n",
      "Epoch 68/180\n",
      "1000/1000 - 3s - loss: 0.2108 - val_loss: 0.2045\n",
      "Epoch 69/180\n",
      "1000/1000 - 3s - loss: 0.2173 - val_loss: 0.2083\n",
      "Epoch 70/180\n",
      "1000/1000 - 3s - loss: 0.2188 - val_loss: 0.2047\n",
      "Epoch 71/180\n",
      "1000/1000 - 3s - loss: 0.2206 - val_loss: 0.2118\n",
      "Epoch 72/180\n",
      "1000/1000 - 3s - loss: 0.2140 - val_loss: 0.2051\n",
      "Epoch 73/180\n",
      "1000/1000 - 3s - loss: 0.2120 - val_loss: 0.2038\n",
      "Epoch 74/180\n",
      "1000/1000 - 3s - loss: 0.2128 - val_loss: 0.2057\n",
      "Epoch 75/180\n",
      "1000/1000 - 3s - loss: 0.2078 - val_loss: 0.2046\n",
      "Epoch 78/180\n",
      "1000/1000 - 3s - loss: 0.2218 - val_loss: 0.2197\n",
      "Epoch 79/180\n",
      "1000/1000 - 3s - loss: 0.2058 - val_loss: 0.2051\n",
      "Epoch 80/180\n",
      "1000/1000 - 3s - loss: 0.2079 - val_loss: 0.2030\n",
      "Epoch 81/180\n",
      "1000/1000 - 3s - loss: 0.2088 - val_loss: 0.2021\n",
      "Epoch 82/180\n",
      "1000/1000 - 3s - loss: 0.2088 - val_loss: 0.2047\n",
      "Epoch 83/180\n",
      "1000/1000 - 3s - loss: 0.2108 - val_loss: 0.2056\n",
      "Epoch 84/180\n",
      "1000/1000 - 3s - loss: 0.2057 - val_loss: 0.2233\n",
      "Epoch 85/180\n",
      "1000/1000 - 3s - loss: 0.2054 - val_loss: 0.2031\n",
      "Epoch 86/180\n",
      "1000/1000 - 3s - loss: 0.2088 - val_loss: 0.2052\n",
      "Epoch 87/180\n",
      "1000/1000 - 3s - loss: 0.2061 - val_loss: 0.2083\n",
      "Epoch 88/180\n",
      "1000/1000 - 3s - loss: 0.2063 - val_loss: 0.2108\n",
      "Epoch 89/180\n",
      "1000/1000 - 3s - loss: 0.2003 - val_loss: 0.2031\n",
      "Epoch 90/180\n",
      "1000/1000 - 3s - loss: 0.2021 - val_loss: 0.2011\n",
      "Epoch 91/180\n",
      "1000/1000 - 3s - loss: 0.2035 - val_loss: 0.2034\n",
      "Epoch 92/180\n",
      "1000/1000 - 3s - loss: 0.2074 - val_loss: 0.2002\n",
      "Epoch 93/180\n",
      "1000/1000 - 3s - loss: 0.1966 - val_loss: 0.2001\n",
      "Epoch 94/180\n",
      "1000/1000 - 3s - loss: 0.2025 - val_loss: 0.2008\n",
      "Epoch 95/180\n",
      "1000/1000 - 3s - loss: 0.2103 - val_loss: 0.2100\n",
      "Epoch 96/180\n",
      "1000/1000 - 3s - loss: 0.2012 - val_loss: 0.2239\n",
      "Epoch 97/180\n",
      "1000/1000 - 3s - loss: 0.2054 - val_loss: 0.2002\n",
      "Epoch 98/180\n",
      "1000/1000 - 3s - loss: 0.2067 - val_loss: 0.2008\n",
      "Epoch 99/180\n",
      "1000/1000 - 3s - loss: 0.2012 - val_loss: 0.2026\n",
      "Epoch 100/180\n",
      "1000/1000 - 3s - loss: 0.2010 - val_loss: 0.2171\n",
      "Epoch 101/180\n",
      "1000/1000 - 3s - loss: 0.1980 - val_loss: 0.1999\n",
      "Epoch 102/180\n",
      "1000/1000 - 3s - loss: 0.1947 - val_loss: 0.2024\n",
      "Epoch 103/180\n",
      "1000/1000 - 3s - loss: 0.2041 - val_loss: 0.2054\n",
      "Epoch 104/180\n",
      "1000/1000 - 3s - loss: 0.1966 - val_loss: 0.2068\n",
      "Epoch 105/180\n",
      "1000/1000 - 3s - loss: 0.1999 - val_loss: 0.2010\n",
      "Epoch 106/180\n",
      "1000/1000 - 3s - loss: 0.2010 - val_loss: 0.1996\n",
      "Epoch 107/180\n",
      "1000/1000 - 3s - loss: 0.1998 - val_loss: 0.2015\n",
      "Epoch 108/180\n",
      "1000/1000 - 3s - loss: 0.1935 - val_loss: 0.2010\n",
      "Epoch 109/180\n",
      "1000/1000 - 3s - loss: 0.1963 - val_loss: 0.1990\n",
      "Epoch 110/180\n",
      "1000/1000 - 3s - loss: 0.1974 - val_loss: 0.1991\n",
      "Epoch 111/180\n",
      "1000/1000 - 3s - loss: 0.1978 - val_loss: 0.2029\n",
      "Epoch 112/180\n",
      "1000/1000 - 3s - loss: 0.2033 - val_loss: 0.2017\n",
      "Epoch 113/180\n",
      "1000/1000 - 3s - loss: 0.2027 - val_loss: 0.1992\n",
      "Epoch 114/180\n",
      "1000/1000 - 3s - loss: 0.1954 - val_loss: 0.2006\n",
      "Epoch 115/180\n",
      "1000/1000 - 3s - loss: 0.1977 - val_loss: 0.1985\n",
      "Epoch 116/180\n",
      "1000/1000 - 3s - loss: 0.1957 - val_loss: 0.2002\n",
      "Epoch 117/180\n",
      "1000/1000 - 3s - loss: 0.1943 - val_loss: 0.1989\n",
      "Epoch 118/180\n",
      "1000/1000 - 3s - loss: 0.1943 - val_loss: 0.1982\n",
      "Epoch 119/180\n",
      "1000/1000 - 3s - loss: 0.1949 - val_loss: 0.1977\n",
      "Epoch 120/180\n",
      "1000/1000 - 3s - loss: 0.1956 - val_loss: 0.1971\n",
      "Epoch 121/180\n",
      "1000/1000 - 3s - loss: 0.2005 - val_loss: 0.1974\n",
      "Epoch 122/180\n",
      "1000/1000 - 3s - loss: 0.2033 - val_loss: 0.2037\n",
      "Epoch 123/180\n",
      "1000/1000 - 3s - loss: 0.1983 - val_loss: 0.2040\n",
      "Epoch 124/180\n",
      "1000/1000 - 3s - loss: 0.1941 - val_loss: 0.1980\n",
      "Epoch 125/180\n",
      "1000/1000 - 3s - loss: 0.1917 - val_loss: 0.1964\n",
      "Epoch 126/180\n",
      "1000/1000 - 3s - loss: 0.1997 - val_loss: 0.1975\n",
      "Epoch 127/180\n",
      "1000/1000 - 3s - loss: 0.1983 - val_loss: 0.1987\n",
      "Epoch 128/180\n",
      "1000/1000 - 3s - loss: 0.1946 - val_loss: 0.1984\n",
      "Epoch 129/180\n",
      "1000/1000 - 3s - loss: 0.1911 - val_loss: 0.1968\n",
      "Epoch 130/180\n",
      "1000/1000 - 3s - loss: 0.1943 - val_loss: 0.1958\n",
      "Epoch 131/180\n",
      "1000/1000 - 3s - loss: 0.2037 - val_loss: 0.1961\n",
      "Epoch 132/180\n",
      "1000/1000 - 3s - loss: 0.1877 - val_loss: 0.1975\n",
      "Epoch 133/180\n",
      "1000/1000 - 3s - loss: 0.1867 - val_loss: 0.1967\n",
      "Epoch 134/180\n",
      "1000/1000 - 3s - loss: 0.1920 - val_loss: 0.1953\n",
      "Epoch 135/180\n",
      "1000/1000 - 3s - loss: 0.1977 - val_loss: 0.1975\n",
      "Epoch 136/180\n",
      "1000/1000 - 3s - loss: 0.1993 - val_loss: 0.1979\n",
      "Epoch 137/180\n",
      "1000/1000 - 3s - loss: 0.1883 - val_loss: 0.1952\n",
      "Epoch 138/180\n",
      "1000/1000 - 3s - loss: 0.1861 - val_loss: 0.1956\n",
      "Epoch 139/180\n",
      "1000/1000 - 3s - loss: 0.1914 - val_loss: 0.1980\n",
      "Epoch 140/180\n",
      "1000/1000 - 3s - loss: 0.2002 - val_loss: 0.1941\n",
      "Epoch 141/180\n",
      "1000/1000 - 3s - loss: 0.1898 - val_loss: 0.1953\n",
      "Epoch 142/180\n",
      "1000/1000 - 3s - loss: 0.1876 - val_loss: 0.1948\n",
      "Epoch 143/180\n",
      "1000/1000 - 3s - loss: 0.1926 - val_loss: 0.1976\n",
      "Epoch 144/180\n",
      "1000/1000 - 3s - loss: 0.1878 - val_loss: 0.1940\n",
      "Epoch 145/180\n",
      "1000/1000 - 3s - loss: 0.1861 - val_loss: 0.1938\n",
      "Epoch 146/180\n",
      "1000/1000 - 3s - loss: 0.1877 - val_loss: 0.1933\n",
      "Epoch 147/180\n",
      "1000/1000 - 3s - loss: 0.1864 - val_loss: 0.1948\n",
      "Epoch 148/180\n",
      "1000/1000 - 3s - loss: 0.1890 - val_loss: 0.1929\n",
      "Epoch 149/180\n",
      "1000/1000 - 3s - loss: 0.1888 - val_loss: 0.1938\n",
      "Epoch 150/180\n",
      "1000/1000 - 3s - loss: 0.1852 - val_loss: 0.1943\n",
      "Epoch 151/180\n",
      "1000/1000 - 3s - loss: 0.1841 - val_loss: 0.1937\n",
      "Epoch 152/180\n",
      "1000/1000 - 3s - loss: 0.1934 - val_loss: 0.1924\n",
      "Epoch 153/180\n",
      "1000/1000 - 3s - loss: 0.1861 - val_loss: 0.1926\n",
      "Epoch 154/180\n",
      "1000/1000 - 3s - loss: 0.1889 - val_loss: 0.1926\n",
      "Epoch 155/180\n",
      "1000/1000 - 3s - loss: 0.1903 - val_loss: 0.1941\n",
      "Epoch 156/180\n",
      "1000/1000 - 3s - loss: 0.1915 - val_loss: 0.1930\n",
      "Epoch 157/180\n",
      "1000/1000 - 3s - loss: 0.1863 - val_loss: 0.1934\n",
      "Epoch 158/180\n",
      "1000/1000 - 3s - loss: 0.1925 - val_loss: 0.1928\n",
      "Epoch 159/180\n",
      "1000/1000 - 3s - loss: 0.1884 - val_loss: 0.1947\n",
      "Epoch 160/180\n",
      "1000/1000 - 3s - loss: 0.1862 - val_loss: 0.1934\n",
      "Epoch 161/180\n",
      "1000/1000 - 3s - loss: 0.1847 - val_loss: 0.1922\n",
      "Epoch 162/180\n",
      "1000/1000 - 3s - loss: 0.1854 - val_loss: 0.1931\n",
      "Epoch 163/180\n",
      "1000/1000 - 3s - loss: 0.1915 - val_loss: 0.1980\n",
      "Epoch 164/180\n",
      "1000/1000 - 3s - loss: 0.1863 - val_loss: 0.1931\n",
      "Epoch 165/180\n",
      "1000/1000 - 3s - loss: 0.1888 - val_loss: 0.1926\n",
      "Epoch 166/180\n",
      "1000/1000 - 3s - loss: 0.1839 - val_loss: 0.1920\n",
      "Epoch 167/180\n",
      "1000/1000 - 3s - loss: 0.1932 - val_loss: 0.1966\n",
      "Epoch 168/180\n",
      "1000/1000 - 3s - loss: 0.1840 - val_loss: 0.1920\n",
      "Epoch 169/180\n",
      "1000/1000 - 3s - loss: 0.1843 - val_loss: 0.1921\n",
      "Epoch 170/180\n",
      "1000/1000 - 3s - loss: 0.1848 - val_loss: 0.1925\n",
      "Epoch 171/180\n",
      "1000/1000 - 3s - loss: 0.1923 - val_loss: 0.1974\n",
      "Epoch 172/180\n",
      "1000/1000 - 3s - loss: 0.1853 - val_loss: 0.1917\n",
      "Epoch 173/180\n",
      "1000/1000 - 3s - loss: 0.1806 - val_loss: 0.1916\n",
      "Epoch 174/180\n",
      "1000/1000 - 3s - loss: 0.1842 - val_loss: 0.1915\n",
      "Epoch 175/180\n",
      "1000/1000 - 3s - loss: 0.1813 - val_loss: 0.1923\n",
      "Epoch 176/180\n",
      "1000/1000 - 3s - loss: 0.1832 - val_loss: 0.1927\n",
      "Epoch 177/180\n",
      "1000/1000 - 3s - loss: 0.1832 - val_loss: 0.1909\n",
      "Epoch 178/180\n",
      "1000/1000 - 3s - loss: 0.1809 - val_loss: 0.1912\n",
      "Epoch 179/180\n",
      "1000/1000 - 3s - loss: 0.1853 - val_loss: 0.1917\n",
      "Epoch 180/180\n",
      "1000/1000 - 3s - loss: 0.1864 - val_loss: 0.1904\n",
      "INFO:tensorflow:Assets written to: model_2/assets\n",
      "INFO:tensorflow:Assets written to: encoder_2/assets\n",
      "Our training dataset shape is (1000, 4000, 1)\n",
      "Our validation dataset shape is (250, 4000, 1)\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/180\n",
      "1000/1000 - 8s - loss: 40.7699 - val_loss: 0.9863\n",
      "Epoch 2/180\n",
      "1000/1000 - 3s - loss: 0.5661 - val_loss: 0.5482\n",
      "Epoch 3/180\n",
      "1000/1000 - 3s - loss: 0.3522 - val_loss: 0.4566\n",
      "Epoch 4/180\n",
      "1000/1000 - 3s - loss: 0.2818 - val_loss: 0.3805\n",
      "Epoch 5/180\n",
      "1000/1000 - 3s - loss: 0.2581 - val_loss: 0.3525\n",
      "Epoch 6/180\n",
      "1000/1000 - 3s - loss: 0.2586 - val_loss: 0.2611\n",
      "Epoch 7/180\n",
      "1000/1000 - 3s - loss: 0.2649 - val_loss: 0.2439\n",
      "Epoch 8/180\n",
      "1000/1000 - 3s - loss: 0.2796 - val_loss: 0.2570\n",
      "Epoch 9/180\n",
      "1000/1000 - 3s - loss: 0.2491 - val_loss: 0.2278\n",
      "Epoch 10/180\n",
      "1000/1000 - 3s - loss: 0.2732 - val_loss: 0.3215\n",
      "Epoch 11/180\n",
      "1000/1000 - 3s - loss: 0.2749 - val_loss: 0.2412\n",
      "Epoch 12/180\n",
      "1000/1000 - 3s - loss: 0.2539 - val_loss: 0.2376\n",
      "Epoch 13/180\n",
      "1000/1000 - 3s - loss: 0.2480 - val_loss: 0.2224\n",
      "Epoch 14/180\n",
      "1000/1000 - 3s - loss: 0.2574 - val_loss: 0.2593\n",
      "Epoch 15/180\n",
      "1000/1000 - 3s - loss: 0.2741 - val_loss: 0.2369\n",
      "Epoch 16/180\n",
      "1000/1000 - 3s - loss: 0.2344 - val_loss: 0.2225\n",
      "Epoch 17/180\n",
      "1000/1000 - 3s - loss: 0.2404 - val_loss: 0.2305\n",
      "Epoch 18/180\n",
      "1000/1000 - 3s - loss: 0.2345 - val_loss: 0.2159\n",
      "Epoch 19/180\n",
      "1000/1000 - 3s - loss: 0.2669 - val_loss: 0.2360\n",
      "Epoch 20/180\n",
      "1000/1000 - 3s - loss: 0.2699 - val_loss: 0.2353\n",
      "Epoch 21/180\n",
      "1000/1000 - 3s - loss: 0.2304 - val_loss: 0.2192\n",
      "Epoch 22/180\n",
      "1000/1000 - 3s - loss: 0.2523 - val_loss: 0.2432\n",
      "Epoch 23/180\n",
      "1000/1000 - 3s - loss: 0.2744 - val_loss: 0.2624\n",
      "Epoch 24/180\n",
      "1000/1000 - 3s - loss: 0.2302 - val_loss: 0.2154\n",
      "Epoch 25/180\n",
      "1000/1000 - 3s - loss: 0.2319 - val_loss: 0.2139\n",
      "Epoch 26/180\n",
      "1000/1000 - 3s - loss: 0.2352 - val_loss: 0.2271\n",
      "Epoch 27/180\n",
      "1000/1000 - 3s - loss: 0.2301 - val_loss: 0.2185\n",
      "Epoch 28/180\n",
      "1000/1000 - 3s - loss: 0.2251 - val_loss: 0.2164\n",
      "Epoch 29/180\n",
      "1000/1000 - 3s - loss: 0.2250 - val_loss: 0.2157\n",
      "Epoch 30/180\n",
      "1000/1000 - 3s - loss: 0.2181 - val_loss: 0.2261\n",
      "Epoch 31/180\n",
      "1000/1000 - 3s - loss: 0.2436 - val_loss: 0.2396\n",
      "Epoch 32/180\n",
      "1000/1000 - 3s - loss: 0.2140 - val_loss: 0.2204\n",
      "Epoch 33/180\n",
      "1000/1000 - 3s - loss: 0.2159 - val_loss: 0.2194\n",
      "Epoch 34/180\n",
      "1000/1000 - 3s - loss: 0.2180 - val_loss: 0.2166\n",
      "Epoch 35/180\n",
      "1000/1000 - 3s - loss: 0.2261 - val_loss: 0.2154\n",
      "Epoch 36/180\n",
      "1000/1000 - 3s - loss: 0.2097 - val_loss: 0.2153\n",
      "Epoch 37/180\n",
      "1000/1000 - 3s - loss: 0.2127 - val_loss: 0.2072\n",
      "Epoch 38/180\n",
      "1000/1000 - 3s - loss: 0.2189 - val_loss: 0.2176\n",
      "Epoch 39/180\n",
      "1000/1000 - 3s - loss: 0.2235 - val_loss: 0.2089\n",
      "Epoch 40/180\n",
      "1000/1000 - 3s - loss: 0.2071 - val_loss: 0.2056\n",
      "Epoch 41/180\n",
      "1000/1000 - 3s - loss: 0.2064 - val_loss: 0.2055\n",
      "Epoch 42/180\n",
      "1000/1000 - 3s - loss: 0.2123 - val_loss: 0.2155\n",
      "Epoch 43/180\n",
      "1000/1000 - 3s - loss: 0.2145 - val_loss: 0.2053\n",
      "Epoch 44/180\n",
      "1000/1000 - 3s - loss: 0.1974 - val_loss: 0.2032\n",
      "Epoch 45/180\n",
      "1000/1000 - 3s - loss: 0.2049 - val_loss: 0.2053\n",
      "Epoch 46/180\n",
      "1000/1000 - 3s - loss: 0.2182 - val_loss: 0.2147\n",
      "Epoch 47/180\n",
      "1000/1000 - 3s - loss: 0.2302 - val_loss: 0.2254\n",
      "Epoch 48/180\n",
      "1000/1000 - 3s - loss: 0.2105 - val_loss: 0.2154\n",
      "Epoch 49/180\n",
      "1000/1000 - 3s - loss: 0.1965 - val_loss: 0.1993\n",
      "Epoch 50/180\n",
      "1000/1000 - 3s - loss: 0.2049 - val_loss: 0.2015\n",
      "Epoch 51/180\n",
      "1000/1000 - 3s - loss: 0.2047 - val_loss: 0.2233\n",
      "Epoch 52/180\n",
      "1000/1000 - 3s - loss: 0.1990 - val_loss: 0.2124\n",
      "Epoch 53/180\n",
      "1000/1000 - 3s - loss: 0.1967 - val_loss: 0.2012\n",
      "Epoch 54/180\n",
      "1000/1000 - 3s - loss: 0.2074 - val_loss: 0.2127\n",
      "Epoch 55/180\n",
      "1000/1000 - 3s - loss: 0.1970 - val_loss: 0.2067\n",
      "Epoch 56/180\n",
      "1000/1000 - 3s - loss: 0.2149 - val_loss: 0.1978\n",
      "Epoch 57/180\n",
      "1000/1000 - 3s - loss: 0.1887 - val_loss: 0.1996\n",
      "Epoch 58/180\n",
      "1000/1000 - 3s - loss: 0.1863 - val_loss: 0.1956\n",
      "Epoch 59/180\n",
      "1000/1000 - 3s - loss: 0.2034 - val_loss: 0.2033\n",
      "Epoch 60/180\n",
      "1000/1000 - 3s - loss: 0.1903 - val_loss: 0.2050\n",
      "Epoch 61/180\n",
      "1000/1000 - 3s - loss: 0.1880 - val_loss: 0.1980\n",
      "Epoch 62/180\n",
      "1000/1000 - 3s - loss: 0.1907 - val_loss: 0.1951\n",
      "Epoch 63/180\n",
      "1000/1000 - 3s - loss: 0.1902 - val_loss: 0.1983\n",
      "Epoch 64/180\n",
      "1000/1000 - 3s - loss: 0.1978 - val_loss: 0.1968\n",
      "Epoch 65/180\n",
      "1000/1000 - 3s - loss: 0.1857 - val_loss: 0.1931\n",
      "Epoch 66/180\n",
      "1000/1000 - 3s - loss: 0.1993 - val_loss: 0.1988\n",
      "Epoch 67/180\n",
      "1000/1000 - 3s - loss: 0.1900 - val_loss: 0.1971\n",
      "Epoch 68/180\n",
      "1000/1000 - 3s - loss: 0.1805 - val_loss: 0.1928\n",
      "Epoch 69/180\n",
      "1000/1000 - 3s - loss: 0.1890 - val_loss: 0.1962\n",
      "Epoch 70/180\n",
      "1000/1000 - 3s - loss: 0.2055 - val_loss: 0.1918\n",
      "Epoch 71/180\n",
      "1000/1000 - 3s - loss: 0.1946 - val_loss: 0.2136\n",
      "Epoch 72/180\n",
      "1000/1000 - 3s - loss: 0.1861 - val_loss: 0.1924\n",
      "Epoch 73/180\n",
      "1000/1000 - 3s - loss: 0.1829 - val_loss: 0.1913\n",
      "Epoch 74/180\n",
      "1000/1000 - 3s - loss: 0.1850 - val_loss: 0.1988\n",
      "Epoch 75/180\n",
      "1000/1000 - 3s - loss: 0.1861 - val_loss: 0.1948\n",
      "Epoch 76/180\n",
      "1000/1000 - 3s - loss: 0.1849 - val_loss: 0.1915\n",
      "Epoch 77/180\n",
      "1000/1000 - 3s - loss: 0.1790 - val_loss: 0.1919\n",
      "Epoch 78/180\n",
      "1000/1000 - 3s - loss: 0.1849 - val_loss: 0.2062\n",
      "Epoch 79/180\n",
      "1000/1000 - 3s - loss: 0.1811 - val_loss: 0.1944\n",
      "Epoch 80/180\n",
      "1000/1000 - 3s - loss: 0.1765 - val_loss: 0.1922\n",
      "Epoch 81/180\n",
      "1000/1000 - 3s - loss: 0.1796 - val_loss: 0.1901\n",
      "Epoch 82/180\n",
      "1000/1000 - 3s - loss: 0.1773 - val_loss: 0.1927\n",
      "Epoch 83/180\n",
      "1000/1000 - 3s - loss: 0.1886 - val_loss: 0.1977\n",
      "Epoch 84/180\n",
      "1000/1000 - 3s - loss: 0.1768 - val_loss: 0.1983\n",
      "Epoch 85/180\n",
      "1000/1000 - 3s - loss: 0.1769 - val_loss: 0.1918\n",
      "Epoch 86/180\n",
      "1000/1000 - 3s - loss: 0.1824 - val_loss: 0.1904\n",
      "Epoch 87/180\n",
      "1000/1000 - 3s - loss: 0.1814 - val_loss: 0.1988\n",
      "Epoch 88/180\n",
      "1000/1000 - 3s - loss: 0.1786 - val_loss: 0.2165\n",
      "Epoch 89/180\n",
      "1000/1000 - 3s - loss: 0.1734 - val_loss: 0.1913\n",
      "Epoch 90/180\n",
      "1000/1000 - 3s - loss: 0.1717 - val_loss: 0.1879\n",
      "Epoch 91/180\n",
      "1000/1000 - 3s - loss: 0.1761 - val_loss: 0.1924\n",
      "Epoch 92/180\n",
      "1000/1000 - 3s - loss: 0.1813 - val_loss: 0.1882\n",
      "Epoch 93/180\n",
      "1000/1000 - 3s - loss: 0.1671 - val_loss: 0.1874\n",
      "Epoch 94/180\n",
      "1000/1000 - 3s - loss: 0.1729 - val_loss: 0.1881\n",
      "Epoch 95/180\n",
      "1000/1000 - 3s - loss: 0.1747 - val_loss: 0.1999\n",
      "Epoch 96/180\n",
      "1000/1000 - 3s - loss: 0.1730 - val_loss: 0.2119\n",
      "Epoch 97/180\n",
      "1000/1000 - 3s - loss: 0.1744 - val_loss: 0.1891\n",
      "Epoch 98/180\n",
      "1000/1000 - 3s - loss: 0.1769 - val_loss: 0.1882\n",
      "Epoch 99/180\n",
      "1000/1000 - 3s - loss: 0.1730 - val_loss: 0.2006\n",
      "Epoch 100/180\n",
      "1000/1000 - 3s - loss: 0.1711 - val_loss: 0.2037\n",
      "Epoch 101/180\n",
      "1000/1000 - 3s - loss: 0.1695 - val_loss: 0.1876\n",
      "Epoch 102/180\n",
      "1000/1000 - 3s - loss: 0.1654 - val_loss: 0.1878\n",
      "Epoch 103/180\n",
      "1000/1000 - 3s - loss: 0.1795 - val_loss: 0.1909\n",
      "Epoch 104/180\n",
      "1000/1000 - 3s - loss: 0.1672 - val_loss: 0.1926\n",
      "Epoch 105/180\n",
      "1000/1000 - 3s - loss: 0.1688 - val_loss: 0.1877\n",
      "Epoch 106/180\n",
      "1000/1000 - 3s - loss: 0.1731 - val_loss: 0.1871\n",
      "Epoch 107/180\n",
      "1000/1000 - 3s - loss: 0.1699 - val_loss: 0.1894\n",
      "Epoch 108/180\n",
      "1000/1000 - 3s - loss: 0.1661 - val_loss: 0.1926\n",
      "Epoch 109/180\n",
      "1000/1000 - 3s - loss: 0.1659 - val_loss: 0.1870\n",
      "Epoch 110/180\n",
      "1000/1000 - 3s - loss: 0.1677 - val_loss: 0.1849\n",
      "Epoch 111/180\n",
      "1000/1000 - 3s - loss: 0.1693 - val_loss: 0.1972\n",
      "Epoch 112/180\n",
      "1000/1000 - 3s - loss: 0.1721 - val_loss: 0.1863\n",
      "Epoch 113/180\n",
      "1000/1000 - 3s - loss: 0.1759 - val_loss: 0.1911\n",
      "Epoch 114/180\n",
      "1000/1000 - 3s - loss: 0.1662 - val_loss: 0.1896\n",
      "Epoch 115/180\n",
      "1000/1000 - 3s - loss: 0.1711 - val_loss: 0.1887\n",
      "Epoch 116/180\n",
      "1000/1000 - 3s - loss: 0.1675 - val_loss: 0.1885\n",
      "Epoch 117/180\n",
      "1000/1000 - 3s - loss: 0.1667 - val_loss: 0.1865\n",
      "Epoch 118/180\n",
      "1000/1000 - 3s - loss: 0.1633 - val_loss: 0.1871\n",
      "Epoch 119/180\n",
      "1000/1000 - 3s - loss: 0.1633 - val_loss: 0.1883\n",
      "Epoch 120/180\n",
      "1000/1000 - 3s - loss: 0.1666 - val_loss: 0.1876\n",
      "Epoch 121/180\n",
      "1000/1000 - 3s - loss: 0.1695 - val_loss: 0.1872\n",
      "Epoch 122/180\n",
      "1000/1000 - 3s - loss: 0.1714 - val_loss: 0.2040\n",
      "Epoch 123/180\n",
      "1000/1000 - 3s - loss: 0.1703 - val_loss: 0.1905\n",
      "Epoch 124/180\n",
      "1000/1000 - 3s - loss: 0.1654 - val_loss: 0.1870\n",
      "Epoch 125/180\n",
      "1000/1000 - 3s - loss: 0.1619 - val_loss: 0.1852\n",
      "Epoch 126/180\n",
      "1000/1000 - 3s - loss: 0.1711 - val_loss: 0.1861\n",
      "Epoch 127/180\n",
      "1000/1000 - 3s - loss: 0.1721 - val_loss: 0.1867\n",
      "Epoch 128/180\n",
      "1000/1000 - 3s - loss: 0.1628 - val_loss: 0.1874\n",
      "Epoch 129/180\n",
      "1000/1000 - 3s - loss: 0.1620 - val_loss: 0.1887\n",
      "Epoch 130/180\n",
      "Restoring model weights from the end of the best epoch.\n",
      "1000/1000 - 3s - loss: 0.1646 - val_loss: 0.1873\n",
      "Epoch 00130: early stopping\n",
      "INFO:tensorflow:Assets written to: model_3/assets\n",
      "INFO:tensorflow:Assets written to: encoder_3/assets\n",
      "Our training dataset shape is (1000, 4000, 1)\n",
      "Our validation dataset shape is (250, 4000, 1)\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/180\n",
      "1000/1000 - 8s - loss: 60.0315 - val_loss: 0.9311\n",
      "Epoch 2/180\n",
      "1000/1000 - 3s - loss: 0.4976 - val_loss: 0.5650\n",
      "Epoch 3/180\n",
      "1000/1000 - 3s - loss: 0.3401 - val_loss: 0.4704\n",
      "Epoch 4/180\n",
      "1000/1000 - 3s - loss: 0.2808 - val_loss: 0.4027\n",
      "Epoch 5/180\n",
      "1000/1000 - 3s - loss: 0.2637 - val_loss: 0.4222\n",
      "Epoch 6/180\n",
      "1000/1000 - 3s - loss: 0.2758 - val_loss: 0.2910\n",
      "Epoch 7/180\n",
      "1000/1000 - 4s - loss: 0.2927 - val_loss: 0.2336\n",
      "Epoch 8/180\n",
      "1000/1000 - 3s - loss: 0.2834 - val_loss: 0.2680\n",
      "Epoch 9/180\n",
      "1000/1000 - 3s - loss: 0.2602 - val_loss: 0.2382\n",
      "Epoch 10/180\n",
      "1000/1000 - 3s - loss: 0.2724 - val_loss: 0.2647\n",
      "Epoch 11/180\n",
      "1000/1000 - 3s - loss: 0.2748 - val_loss: 0.2368\n",
      "Epoch 12/180\n",
      "1000/1000 - 3s - loss: 0.2587 - val_loss: 0.2299\n",
      "Epoch 13/180\n",
      "1000/1000 - 3s - loss: 0.2594 - val_loss: 0.2168\n",
      "Epoch 14/180\n",
      "1000/1000 - 3s - loss: 0.2574 - val_loss: 0.2620\n",
      "Epoch 15/180\n",
      "1000/1000 - 3s - loss: 0.2704 - val_loss: 0.2318\n",
      "Epoch 16/180\n",
      "1000/1000 - 3s - loss: 0.2350 - val_loss: 0.2236\n",
      "Epoch 17/180\n",
      "1000/1000 - 3s - loss: 0.2499 - val_loss: 0.2248\n",
      "Epoch 18/180\n",
      "1000/1000 - 3s - loss: 0.2344 - val_loss: 0.2141\n",
      "Epoch 19/180\n",
      "1000/1000 - 3s - loss: 0.2780 - val_loss: 0.2308\n",
      "Epoch 20/180\n",
      "1000/1000 - 3s - loss: 0.2695 - val_loss: 0.2117\n",
      "Epoch 21/180\n",
      "1000/1000 - 3s - loss: 0.2293 - val_loss: 0.2174\n",
      "Epoch 22/180\n",
      "1000/1000 - 3s - loss: 0.2492 - val_loss: 0.2479\n",
      "Epoch 23/180\n",
      "1000/1000 - 3s - loss: 0.2477 - val_loss: 0.2211\n",
      "Epoch 24/180\n",
      "1000/1000 - 3s - loss: 0.2283 - val_loss: 0.2182\n",
      "Epoch 25/180\n",
      "1000/1000 - 3s - loss: 0.2325 - val_loss: 0.2086\n",
      "Epoch 26/180\n",
      "1000/1000 - 3s - loss: 0.2314 - val_loss: 0.2199\n",
      "Epoch 27/180\n",
      "1000/1000 - 3s - loss: 0.2310 - val_loss: 0.2265\n",
      "Epoch 28/180\n",
      "1000/1000 - 3s - loss: 0.2281 - val_loss: 0.2122\n",
      "Epoch 29/180\n",
      "1000/1000 - 3s - loss: 0.2247 - val_loss: 0.2102\n",
      "Epoch 30/180\n",
      "1000/1000 - 3s - loss: 0.2169 - val_loss: 0.2276\n",
      "Epoch 31/180\n",
      "1000/1000 - 3s - loss: 0.2506 - val_loss: 0.2161\n",
      "Epoch 32/180\n",
      "1000/1000 - 3s - loss: 0.2175 - val_loss: 0.2162\n",
      "Epoch 33/180\n",
      "1000/1000 - 3s - loss: 0.2200 - val_loss: 0.2143\n",
      "Epoch 34/180\n",
      "1000/1000 - 3s - loss: 0.2226 - val_loss: 0.2104\n",
      "Epoch 35/180\n",
      "1000/1000 - 3s - loss: 0.2287 - val_loss: 0.2192\n",
      "Epoch 36/180\n",
      "1000/1000 - 3s - loss: 0.2179 - val_loss: 0.2268\n",
      "Epoch 37/180\n",
      "1000/1000 - 3s - loss: 0.2172 - val_loss: 0.2054\n",
      "Epoch 38/180\n",
      "1000/1000 - 3s - loss: 0.2233 - val_loss: 0.2181\n",
      "Epoch 39/180\n",
      "1000/1000 - 3s - loss: 0.2187 - val_loss: 0.2095\n",
      "Epoch 40/180\n",
      "1000/1000 - 3s - loss: 0.2122 - val_loss: 0.2048\n",
      "Epoch 41/180\n",
      "1000/1000 - 3s - loss: 0.2132 - val_loss: 0.2040\n",
      "Epoch 42/180\n",
      "1000/1000 - 3s - loss: 0.2176 - val_loss: 0.2108\n",
      "Epoch 43/180\n",
      "1000/1000 - 3s - loss: 0.2209 - val_loss: 0.2046\n",
      "Epoch 44/180\n",
      "1000/1000 - 3s - loss: 0.2066 - val_loss: 0.2033\n",
      "Epoch 45/180\n",
      "1000/1000 - 3s - loss: 0.2120 - val_loss: 0.2153\n",
      "Epoch 46/180\n",
      "1000/1000 - 3s - loss: 0.2205 - val_loss: 0.2201\n",
      "Epoch 47/180\n",
      "1000/1000 - 3s - loss: 0.2411 - val_loss: 0.2143\n",
      "Epoch 48/180\n",
      "1000/1000 - 3s - loss: 0.2170 - val_loss: 0.2171\n",
      "Epoch 49/180\n",
      "1000/1000 - 3s - loss: 0.2049 - val_loss: 0.2004\n",
      "Epoch 50/180\n",
      "1000/1000 - 3s - loss: 0.2162 - val_loss: 0.2023\n",
      "Epoch 51/180\n",
      "1000/1000 - 3s - loss: 0.2099 - val_loss: 0.2073\n",
      "Epoch 52/180\n",
      "1000/1000 - 3s - loss: 0.2055 - val_loss: 0.2063\n",
      "Epoch 53/180\n",
      "1000/1000 - 3s - loss: 0.2016 - val_loss: 0.2047\n",
      "Epoch 54/180\n",
      "1000/1000 - 3s - loss: 0.2161 - val_loss: 0.2249\n",
      "Epoch 55/180\n",
      "1000/1000 - 3s - loss: 0.2076 - val_loss: 0.2036\n",
      "Epoch 56/180\n",
      "1000/1000 - 3s - loss: 0.2190 - val_loss: 0.2025\n",
      "Epoch 57/180\n",
      "1000/1000 - 3s - loss: 0.1982 - val_loss: 0.1985\n",
      "Epoch 58/180\n",
      "1000/1000 - 3s - loss: 0.1959 - val_loss: 0.2004\n",
      "Epoch 59/180\n",
      "1000/1000 - 3s - loss: 0.2059 - val_loss: 0.2187\n",
      "Epoch 60/180\n",
      "1000/1000 - 3s - loss: 0.2019 - val_loss: 0.1981\n",
      "Epoch 61/180\n",
      "1000/1000 - 3s - loss: 0.1972 - val_loss: 0.1989\n",
      "Epoch 62/180\n",
      "1000/1000 - 3s - loss: 0.1974 - val_loss: 0.1965\n",
      "Epoch 63/180\n",
      "1000/1000 - 3s - loss: 0.2027 - val_loss: 0.2005\n",
      "Epoch 64/180\n",
      "1000/1000 - 3s - loss: 0.2051 - val_loss: 0.1957\n",
      "Epoch 65/180\n",
      "1000/1000 - 3s - loss: 0.1911 - val_loss: 0.1936\n",
      "Epoch 66/180\n",
      "1000/1000 - 3s - loss: 0.2079 - val_loss: 0.1996\n",
      "Epoch 67/180\n",
      "1000/1000 - 3s - loss: 0.1982 - val_loss: 0.2026\n",
      "Epoch 68/180\n",
      "1000/1000 - 3s - loss: 0.1897 - val_loss: 0.1941\n",
      "Epoch 69/180\n",
      "1000/1000 - 3s - loss: 0.2008 - val_loss: 0.1958\n",
      "Epoch 70/180\n",
      "1000/1000 - 3s - loss: 0.2078 - val_loss: 0.1948\n",
      "Epoch 71/180\n",
      "1000/1000 - 3s - loss: 0.2005 - val_loss: 0.2130\n",
      "Epoch 72/180\n",
      "1000/1000 - 3s - loss: 0.1991 - val_loss: 0.1939\n",
      "Epoch 73/180\n",
      "1000/1000 - 3s - loss: 0.1911 - val_loss: 0.1936\n",
      "Epoch 74/180\n",
      "1000/1000 - 3s - loss: 0.1957 - val_loss: 0.1951\n",
      "Epoch 75/180\n",
      "1000/1000 - 3s - loss: 0.1976 - val_loss: 0.1953\n",
      "Epoch 76/180\n",
      "1000/1000 - 3s - loss: 0.1922 - val_loss: 0.1942\n",
      "Epoch 77/180\n",
      "1000/1000 - 3s - loss: 0.1889 - val_loss: 0.1929\n",
      "Epoch 78/180\n",
      "1000/1000 - 3s - loss: 0.1992 - val_loss: 0.1982\n",
      "Epoch 79/180\n",
      "1000/1000 - 3s - loss: 0.1860 - val_loss: 0.1937\n",
      "Epoch 80/180\n",
      "1000/1000 - 3s - loss: 0.1855 - val_loss: 0.1919\n",
      "Epoch 81/180\n",
      "1000/1000 - 3s - loss: 0.1912 - val_loss: 0.1928\n",
      "Epoch 82/180\n",
      "1000/1000 - 3s - loss: 0.1897 - val_loss: 0.1909\n",
      "Epoch 83/180\n",
      "1000/1000 - 3s - loss: 0.1956 - val_loss: 0.1947\n",
      "Epoch 84/180\n",
      "1000/1000 - 3s - loss: 0.1853 - val_loss: 0.1921\n",
      "Epoch 85/180\n",
      "1000/1000 - 3s - loss: 0.1849 - val_loss: 0.1910\n",
      "Epoch 86/180\n",
      "1000/1000 - 3s - loss: 0.1933 - val_loss: 0.1926\n",
      "Epoch 87/180\n",
      "1000/1000 - 3s - loss: 0.1910 - val_loss: 0.1973\n",
      "Epoch 88/180\n",
      "1000/1000 - 3s - loss: 0.1847 - val_loss: 0.1920\n",
      "Epoch 89/180\n",
      "1000/1000 - 3s - loss: 0.1779 - val_loss: 0.1896\n",
      "Epoch 90/180\n",
      "1000/1000 - 3s - loss: 0.1791 - val_loss: 0.1886\n",
      "Epoch 91/180\n",
      "1000/1000 - 3s - loss: 0.1894 - val_loss: 0.1899\n",
      "Epoch 92/180\n",
      "1000/1000 - 3s - loss: 0.1866 - val_loss: 0.1868\n",
      "Epoch 93/180\n",
      "1000/1000 - 3s - loss: 0.1762 - val_loss: 0.1862\n",
      "Epoch 94/180\n",
      "1000/1000 - 3s - loss: 0.1797 - val_loss: 0.1876\n",
      "Epoch 95/180\n",
      "1000/1000 - 3s - loss: 0.1865 - val_loss: 0.2044\n",
      "Epoch 96/180\n",
      "1000/1000 - 3s - loss: 0.1833 - val_loss: 0.2283\n",
      "Epoch 97/180\n",
      "1000/1000 - 3s - loss: 0.1839 - val_loss: 0.1859\n",
      "Epoch 98/180\n",
      "1000/1000 - 3s - loss: 0.1871 - val_loss: 0.1880\n",
      "Epoch 99/180\n",
      "1000/1000 - 3s - loss: 0.1812 - val_loss: 0.1875\n",
      "Epoch 100/180\n",
      "1000/1000 - 3s - loss: 0.1796 - val_loss: 0.1984\n",
      "Epoch 101/180\n",
      "1000/1000 - 3s - loss: 0.1790 - val_loss: 0.1846\n",
      "Epoch 102/180\n",
      "1000/1000 - 3s - loss: 0.1735 - val_loss: 0.1897\n",
      "Epoch 103/180\n",
      "1000/1000 - 3s - loss: 0.1865 - val_loss: 0.1917\n",
      "Epoch 104/180\n",
      "1000/1000 - 3s - loss: 0.1739 - val_loss: 0.1889\n",
      "Epoch 105/180\n",
      "1000/1000 - 3s - loss: 0.1770 - val_loss: 0.1869\n",
      "Epoch 106/180\n",
      "1000/1000 - 3s - loss: 0.1807 - val_loss: 0.1857\n",
      "Epoch 107/180\n",
      "1000/1000 - 3s - loss: 0.1779 - val_loss: 0.1890\n",
      "Epoch 108/180\n",
      "1000/1000 - 3s - loss: 0.1715 - val_loss: 0.1922\n",
      "Epoch 109/180\n",
      "1000/1000 - 3s - loss: 0.1755 - val_loss: 0.1851\n",
      "Epoch 110/180\n",
      "1000/1000 - 3s - loss: 0.1755 - val_loss: 0.1842\n",
      "Epoch 111/180\n",
      "1000/1000 - 3s - loss: 0.1772 - val_loss: 0.1875\n",
      "Epoch 112/180\n",
      "1000/1000 - 3s - loss: 0.1802 - val_loss: 0.1897\n",
      "Epoch 113/180\n",
      "1000/1000 - 3s - loss: 0.1812 - val_loss: 0.1868\n",
      "Epoch 114/180\n",
      "1000/1000 - 3s - loss: 0.1749 - val_loss: 0.1877\n",
      "Epoch 115/180\n",
      "1000/1000 - 3s - loss: 0.1793 - val_loss: 0.1874\n",
      "Epoch 116/180\n",
      "1000/1000 - 3s - loss: 0.1749 - val_loss: 0.1849\n",
      "Epoch 117/180\n",
      "1000/1000 - 3s - loss: 0.1733 - val_loss: 0.1841\n",
      "Epoch 118/180\n",
      "1000/1000 - 3s - loss: 0.1719 - val_loss: 0.1853\n",
      "Epoch 119/180\n",
      "1000/1000 - 3s - loss: 0.1703 - val_loss: 0.1911\n",
      "Epoch 120/180\n",
      "1000/1000 - 3s - loss: 0.1744 - val_loss: 0.1844\n",
      "Epoch 121/180\n",
      "1000/1000 - 3s - loss: 0.1781 - val_loss: 0.1829\n",
      "Epoch 122/180\n",
      "1000/1000 - 3s - loss: 0.1787 - val_loss: 0.1912\n",
      "Epoch 123/180\n",
      "1000/1000 - 3s - loss: 0.1788 - val_loss: 0.1890\n",
      "Epoch 124/180\n",
      "1000/1000 - 3s - loss: 0.1724 - val_loss: 0.1846\n",
      "Epoch 125/180\n",
      "1000/1000 - 3s - loss: 0.1699 - val_loss: 0.1823\n",
      "Epoch 126/180\n",
      "1000/1000 - 3s - loss: 0.1839 - val_loss: 0.1841\n",
      "Epoch 127/180\n",
      "1000/1000 - 3s - loss: 0.1790 - val_loss: 0.1879\n",
      "Epoch 128/180\n",
      "1000/1000 - 3s - loss: 0.1724 - val_loss: 0.1859\n",
      "Epoch 129/180\n",
      "1000/1000 - 3s - loss: 0.1695 - val_loss: 0.1838\n",
      "Epoch 130/180\n",
      "1000/1000 - 3s - loss: 0.1711 - val_loss: 0.1817\n",
      "Epoch 131/180\n",
      "1000/1000 - 3s - loss: 0.1768 - val_loss: 0.1864\n",
      "Epoch 132/180\n",
      "1000/1000 - 3s - loss: 0.1670 - val_loss: 0.1875\n",
      "Epoch 133/180\n",
      "1000/1000 - 3s - loss: 0.1668 - val_loss: 0.1834\n",
      "Epoch 134/180\n",
      "1000/1000 - 3s - loss: 0.1687 - val_loss: 0.1826\n",
      "Epoch 135/180\n",
      "1000/1000 - 3s - loss: 0.1759 - val_loss: 0.1913\n",
      "Epoch 136/180\n",
      "1000/1000 - 3s - loss: 0.1795 - val_loss: 0.1888\n",
      "Epoch 137/180\n",
      "1000/1000 - 3s - loss: 0.1673 - val_loss: 0.1835\n",
      "Epoch 138/180\n",
      "1000/1000 - 3s - loss: 0.1674 - val_loss: 0.1850\n",
      "Epoch 139/180\n",
      "1000/1000 - 3s - loss: 0.1707 - val_loss: 0.1823\n",
      "Epoch 140/180\n",
      "1000/1000 - 3s - loss: 0.1820 - val_loss: 0.1817\n",
      "Epoch 141/180\n",
      "1000/1000 - 3s - loss: 0.1677 - val_loss: 0.1833\n",
      "Epoch 142/180\n",
      "1000/1000 - 3s - loss: 0.1656 - val_loss: 0.1813\n",
      "Epoch 143/180\n",
      "1000/1000 - 3s - loss: 0.1703 - val_loss: 0.1861\n",
      "Epoch 144/180\n",
      "1000/1000 - 3s - loss: 0.1663 - val_loss: 0.1843\n",
      "Epoch 145/180\n",
      "1000/1000 - 3s - loss: 0.1646 - val_loss: 0.1817\n",
      "Epoch 146/180\n",
      "1000/1000 - 3s - loss: 0.1660 - val_loss: 0.1816\n",
      "Epoch 147/180\n",
      "1000/1000 - 3s - loss: 0.1681 - val_loss: 0.1818\n",
      "Epoch 148/180\n",
      "1000/1000 - 3s - loss: 0.1681 - val_loss: 0.1832\n",
      "Epoch 149/180\n",
      "1000/1000 - 3s - loss: 0.1685 - val_loss: 0.1830\n",
      "Epoch 150/180\n",
      "1000/1000 - 3s - loss: 0.1644 - val_loss: 0.1831\n",
      "Epoch 151/180\n",
      "1000/1000 - 3s - loss: 0.1661 - val_loss: 0.1869\n",
      "Epoch 152/180\n",
      "1000/1000 - 3s - loss: 0.1713 - val_loss: 0.1830\n",
      "Epoch 153/180\n",
      "1000/1000 - 3s - loss: 0.1650 - val_loss: 0.1822\n",
      "Epoch 154/180\n",
      "1000/1000 - 3s - loss: 0.1665 - val_loss: 0.1812\n",
      "Epoch 155/180\n",
      "1000/1000 - 3s - loss: 0.1712 - val_loss: 0.1888\n",
      "Epoch 156/180\n",
      "1000/1000 - 3s - loss: 0.1722 - val_loss: 0.1813\n",
      "Epoch 157/180\n",
      "1000/1000 - 3s - loss: 0.1660 - val_loss: 0.1838\n",
      "Epoch 158/180\n",
      "1000/1000 - 3s - loss: 0.1722 - val_loss: 0.1826\n",
      "Epoch 159/180\n",
      "1000/1000 - 3s - loss: 0.1699 - val_loss: 0.1840\n",
      "Epoch 160/180\n",
      "1000/1000 - 3s - loss: 0.1673 - val_loss: 0.1826\n",
      "Epoch 161/180\n",
      "1000/1000 - 3s - loss: 0.1650 - val_loss: 0.1826\n",
      "Epoch 162/180\n",
      "1000/1000 - 3s - loss: 0.1663 - val_loss: 0.1844\n",
      "Epoch 163/180\n",
      "1000/1000 - 3s - loss: 0.1707 - val_loss: 0.1845\n",
      "Epoch 164/180\n",
      "1000/1000 - 3s - loss: 0.1665 - val_loss: 0.1820\n",
      "Epoch 165/180\n",
      "1000/1000 - 3s - loss: 0.1684 - val_loss: 0.1834\n",
      "Epoch 166/180\n",
      "1000/1000 - 3s - loss: 0.1638 - val_loss: 0.1837\n",
      "Epoch 167/180\n",
      "1000/1000 - 3s - loss: 0.1723 - val_loss: 0.1881\n",
      "Epoch 168/180\n",
      "1000/1000 - 3s - loss: 0.1672 - val_loss: 0.1810\n",
      "Epoch 169/180\n",
      "1000/1000 - 3s - loss: 0.1643 - val_loss: 0.1818\n",
      "Epoch 170/180\n",
      "1000/1000 - 3s - loss: 0.1652 - val_loss: 0.1834\n",
      "Epoch 171/180\n",
      "1000/1000 - 3s - loss: 0.1722 - val_loss: 0.1860\n",
      "Epoch 172/180\n",
      "1000/1000 - 3s - loss: 0.1638 - val_loss: 0.1818\n",
      "Epoch 173/180\n",
      "1000/1000 - 3s - loss: 0.1606 - val_loss: 0.1823\n",
      "Epoch 174/180\n",
      "1000/1000 - 3s - loss: 0.1645 - val_loss: 0.1815\n",
      "Epoch 175/180\n",
      "1000/1000 - 3s - loss: 0.1618 - val_loss: 0.1814\n",
      "Epoch 176/180\n",
      "1000/1000 - 3s - loss: 0.1652 - val_loss: 0.1835\n",
      "Epoch 177/180\n",
      "1000/1000 - 3s - loss: 0.1638 - val_loss: 0.1825\n",
      "Epoch 178/180\n",
      "1000/1000 - 3s - loss: 0.1611 - val_loss: 0.1827\n",
      "Epoch 179/180\n",
      "1000/1000 - 3s - loss: 0.1653 - val_loss: 0.1819\n",
      "Epoch 180/180\n",
      "1000/1000 - 3s - loss: 0.1667 - val_loss: 0.1814\n",
      "INFO:tensorflow:Assets written to: model_4/assets\n",
      "INFO:tensorflow:Assets written to: encoder_4/assets\n",
      "Training completed...\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "def read_data():\n",
    "    train = pd.read_csv('../input/data-without-drift/train_clean.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n",
    "    test  = pd.read_csv('../input/data-without-drift/test_clean.csv', dtype={'time': np.float32, 'signal': np.float32})\n",
    "    sub  = pd.read_csv('../input/liverpool-ion-switching/sample_submission.csv', dtype={'time': np.float32})\n",
    "        \n",
    "    return train, test, sub\n",
    "\n",
    "# create batches of 4000 observations\n",
    "def batching(df, batch_size):\n",
    "    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n",
    "    df['group'] = df['group'].astype(np.uint16)\n",
    "    return df\n",
    "\n",
    "# normalize the data (standard scaler). We can also try other scalers for a better score!\n",
    "def normalize(train, test):\n",
    "    train_input_mean = train.signal.mean()\n",
    "    train_input_sigma = train.signal.std()\n",
    "    train['signal'] = (train.signal - train_input_mean) / train_input_sigma\n",
    "    test['signal'] = (test.signal - train_input_mean) / train_input_sigma\n",
    "    return train, test\n",
    "\n",
    "# get lead and lags features\n",
    "def lag_with_pct_change(df, windows):\n",
    "    for window in windows:    \n",
    "        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n",
    "        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n",
    "    return df\n",
    "\n",
    "# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\n",
    "def run_feat_engineering(df, batch_size):\n",
    "    # create batches\n",
    "    df = batching(df, batch_size = batch_size)\n",
    "    return df\n",
    "\n",
    "# fillna with the mean and select features for training\n",
    "def feature_selection(train, test):\n",
    "    features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)\n",
    "    test = test.replace([np.inf, -np.inf], np.nan)\n",
    "    for feature in features:\n",
    "        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n",
    "        train[feature] = train[feature].fillna(feature_mean)\n",
    "        test[feature] = test[feature].fillna(feature_mean)\n",
    "    return train, test, features\n",
    "\n",
    "\n",
    "# main function to perfrom groupkfold cross validation (we have 1000 vectores of 4000 rows and 8 features (columns)). Going to make 5 groups with this subgroups.\n",
    "def run_cv_model_by_batch(train, test, splits, batch_col, feats, sample_submission, nn_epochs, nn_batch_size):\n",
    "    \n",
    "    seed_everything(SEED)\n",
    "    K.clear_session()\n",
    "    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "    oof_ = np.zeros((len(train)//GROUP_BATCH_SIZE, 32)) # build out of folds matrix with 11 columns, they represent our target variables classes (from 0 to 10)\n",
    "    preds_ = np.zeros((len(test)//GROUP_BATCH_SIZE, 32))\n",
    "    target = ['open_channels']\n",
    "    group = train['group']\n",
    "    kf = GroupKFold(n_splits=5)\n",
    "    splits = [x for x in kf.split(train, train[target], group)]\n",
    "\n",
    "    new_splits = []\n",
    "    for sp in splits:\n",
    "        new_split = []\n",
    "        new_split.append(np.unique(group[sp[0]]))\n",
    "        new_split.append(np.unique(group[sp[1]]))\n",
    "        new_split.append(sp[1])    \n",
    "        new_splits.append(new_split)\n",
    "    # pivot target columns to transform the net to a multiclass classification estructure (you can also leave it in 1 vector with sparsecategoricalcrossentropy loss function)\n",
    "    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n",
    "\n",
    "    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n",
    "    target_cols = ['target_'+str(i) for i in range(11)]\n",
    "    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n",
    "    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n",
    "    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n",
    "    \n",
    "    print(train.shape)\n",
    "\n",
    "    for n_fold, (tr_idx, val_idx, val_orig_idx) in list(enumerate(new_splits))[0:5]:\n",
    "        train_x, _ = train[tr_idx], train_tr[tr_idx]\n",
    "        valid_x, _ = train[val_idx], train_tr[val_idx]\n",
    "        print(f'Our training dataset shape is {train_x.shape}')\n",
    "        print(f'Our validation dataset shape is {valid_x.shape}')\n",
    "\n",
    "        gc.collect()\n",
    "        shape_ = (4000, 1) # input is going to be the number of feature we are using (dimension 2 of 0, 1, 2)\n",
    "        \n",
    "        model, encoder = Classifier(shape_)\n",
    "        # using our lr_schedule function\n",
    "        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "        \n",
    "        tb = TensorBoard(log_dir='logs')\n",
    "        \n",
    "        model.fit(train_x,train_x,\n",
    "                  epochs = nn_epochs,\n",
    "                  callbacks = [cb_lr_schedule, es, tb], # adding custom evaluation metric for each epoch\n",
    "                  batch_size = nn_batch_size,verbose = 2,\n",
    "                  validation_data = (valid_x,valid_x))\n",
    "        \n",
    "        model.save(f'model_{n_fold}')\n",
    "        encoder.save(f'encoder_{n_fold}')\n",
    "        \n",
    "        preds_f = encoder.predict(valid_x)\n",
    "        \n",
    "#         preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n",
    "        \n",
    "        oof_[val_idx,:] += preds_f\n",
    "        \n",
    "        te_preds = encoder.predict(test)\n",
    "        \n",
    "#         te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n",
    "        preds_ += te_preds / SPLITS\n",
    "        \n",
    "    np.save('oof_wavenet_probs.npy', oof_)\n",
    "    np.save('test_wavenet_probs.npy', preds_)\n",
    "\n",
    "# this function run our entire program\n",
    "def run_everything():\n",
    "    \n",
    "    print('Reading Data Started...')\n",
    "    train, test, sample_submission = read_data()\n",
    "    train, test = normalize(train, test)\n",
    "    print('Reading and Normalizing Data Completed')\n",
    "        \n",
    "    print('Creating Features')\n",
    "    print('Feature Engineering Started...')\n",
    "    train = run_feat_engineering(train, batch_size = GROUP_BATCH_SIZE)\n",
    "    test = run_feat_engineering(test, batch_size = GROUP_BATCH_SIZE)\n",
    "    train, test, features = feature_selection(train, test)\n",
    "    display(train.head())\n",
    "    display(test.head())\n",
    "    print('Feature Engineering Completed...')\n",
    "        \n",
    "   \n",
    "    print(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started...')\n",
    "    run_cv_model_by_batch(train, test, SPLITS, 'group', features, sample_submission, EPOCHS, NNBATCHSIZE)\n",
    "    print('Training completed...')\n",
    "        \n",
    "run_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c liverpool-ion-switching -f submission_wavenet.csv -m \"crg proba knn lgbm mlp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('oof_wavenet_probs.npy').shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
